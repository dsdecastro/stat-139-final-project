---
title: "Characterizing the relationship between the religious affiliation and incidences of Covid-19 at U.S. universities."
author: "Daniel de Castro and Laura Appleby"
date: "December 14, 2022"
output: pdf_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, results=TRUE, messages=FALSE)
```

```{r, eval=T, include=F}
library(dplyr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(modelsummary)
library(sandwich)
library(ggplot2)

library(lemon)
knit_print.data.frame <- lemon_print

library(glmnet)
```

```{r, eval=T, include=F}
RMSE <- function(y, yhat) {
  SSE = sum((y-yhat)^2)
  return(sqrt(SSE/length(y)))
}
```

# Introduction

Paragraph expressing our motivations for pursuing this project, a brief analysis plan, and our hypotheses. 

# Description of data and source

Our data for this project comes from three sources: 

1. [NYTimes Covid-19 Data](https://github.com/nytimes/covid-19-data/tree/master/colleges). This is publicly available on GitHub and was the source for some of the NYTimes maps and data visuals during the 2020-2021 era of the pandemic. It includes cases from 2020 - May 2021, and we have specifically selected cases at Universities. This dataset has 1948 entries and includes 2020 cases, 2021 cases, University IPEDS ID, University Name, State, etc. 

2. [IPEDS Data Center](https://nces.ed.gov/ipeds/use-the-data). This is publicly available data on colleges across the globe. It has many possible variables including demographics, admission rates, University affiliation, etc. The IPEDS data center allowed us to select certain Universities and variables. The smallest subset of Universities that included all from the NYTimes database (by IPEDS ID) was 6125 rows, with all US Universities. 

3. [Centers for Disease Control](https://data.cdc.gov/Policy-Surveillance/U-S-State-and-Territorial-Public-Mask-Mandates-Fro/tzyy-aayg). This publicly available data set tracks mask mandates in each state from April 8, 2020, to August 15, 2021. 

4. The presidential elections data from STAT 139 Problem Set 4. Of this data set, we will be particularly focused on the `gap20repub` predictor, which we will use to help characterize the political climate of the state in which each institution is located.  

The data has $n = 1,855$ rows after removing Universities without stats or without matching IPEDS ids.

# Data Cleaning Procedures

Our first step in cleaning the data is to read our data from CSV files into R data frames. The `colleges` data frame stores the NYT data on Covid cases at universities, while the `ipeds` data frame stores the data will most of our predictor variables (university characteristics) taken from IPEDS. We then rename most of the columns in `ipeds` to make them shorter and easier to work with. 

```{r, eval=T, echo = FALSE}
# Read colleges from csv
colleges <- read.csv("data/colleges.csv")[,-c(1,9)]

# Read ipeds from csv, drop unnecessary columns
ipeds <- read.csv("data/ipeds.csv")
ipeds <- ipeds[-c(3, 8, 22)]

# Rename columns of ipeds 
ipeds <- ipeds %>% rename_at('IC2021.Institutional.control.or.affiliation', 
                             ~'control') %>% 
                    rename_at('HD2021.FIPS.state.code', ~'FIPS.state.code') %>% 
                    rename_at('unitid', ~'ipeds_id') %>% 
                    rename_at('IC2021.Religious.affiliation', 
                              ~'religious.affiliation') %>%
                    rename_at('DRVIC2021.Tuition.and.fees..2020.21', 
                              ~'tuition') %>% 
                    rename_at('DRVEF122021.Total.12.month.unduplicated.headcount', 
                              ~'total.headcount') %>% 
                    rename_at('DRVEF122021.Undergraduate.12.month.unduplicated.headcount',
                    ~'undergrad.headcount') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.American.Indian.or.Alaska.Native', 
                              ~'percent.american.native') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.Asian', 
                              ~'percent.asian') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.Black.or.African.American', 
                              ~'percent.black') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.Hispanic.Latino', ~'percent.hispanic.latino') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.Native.Hawaiian.or.Other.Pacific.Islander', 
                              ~'percent.pacific.islander') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.White', 
                              ~'percent.white') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.two.or.more.races', 
                              ~'percent.two.more.races') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.race.ethnicity.unknown', ~'percent.NA.race') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.Nonresident.Alien',
                              ~'percent.nonres.alien') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.women', ~'percent.women') %>% 
                    rename_at('SFA2021.Average.amount.of.grant.and.scholarship.aid.awarded..2020.21', ~'avg.grant.money') %>% 
                    rename_at('DRVGR2021.Graduation.rate..total.cohort', ~'grad.rate') %>% 
                    rename_at('SFA2021.Percent.of.full.time.first.time.undergraduates.awarded.any.financial.aid', 
                              ~'percent.fin.aid') %>% 
                    rename_at('SFA2021.Percent.of.full.time.first.time.undergraduates.awarded.student.loans', 
                              ~'percent.student.loan') %>% 
                    rename_at('IC2021.Occupational', ~'occupational.degree') %>% 
                    rename_at('IC2021.Academic', ~'academic.degree') %>% 
                    rename_at('IC2021.Adult.basic.remedial.or.high.school.equivalent', 
                              ~'hs.equivalent.degree') %>% 
                    rename_at('IC2021.Percent.of.undergraduates..who.are.formally.registered.as.students.with.disabilities..when.percentage.is.more.than.3.percent', 
                              ~'percent.disability') %>% 
                    rename_at('IC2021.NCAA.NAIA.conference.number.football', 
                              ~'NCAA.football') %>% 
                    rename_at('IC2021.Institution.provide.on.campus.housing', 
                              ~'on.campus.housing') %>% 
                    rename_at('IC2021.Total.dormitory.capacity', 
                              ~'dorm.capacity') %>% 
                    rename_at('IC2021.Typical.room.charge.for.academic.year', 
                              ~'dorm.room.price')
```

Next, we merge the `colleges` and `ipeds` data frames on the `ipeds_id` column and remove institutions with no IPEDS data. We then create the `religious`, `catholic`, and `private` columns, which are simply indicators for whether an institution has any religious affiliation, whether it has a catholic affiliation, and whether it is a private university. Finally, we drop any unnecessary or redundant columns from the data frame.

```{r, eval=T, echo = FALSE}
# Merge data frames, and remove duplicated "state" column 
md <- merge(ipeds, colleges, by="ipeds_id")[,-32]

# Remove institutions with no ipeds data (a lot of NAs)
md <- md[!(md$control == ""),]

# Create column 'religious' 
md$religious <- "Yes"
md[md$religious.affiliation == "Not applicable", ]$religious <- "No"

# Create column 'catholic'
md$catholic <- "No"
md[md$religious.affiliation == "Roman Catholic",]$catholic <- "Yes"

# Create column 'private' 
md$private <- "No"
md$private[md$control == "Private not-for-profit (no religious affiliation)" | 
             md$control == "Private not-for-profit (religious affiliation)"] <- 
  "Yes"

# Drop column 'control', 'academic.degree', 'dorm.room.price'
md <- md[,-c(4,24,30)]

# Set 'dorm.capacity' to 0 if no on.campus.housing
md$dorm.capacity[md$on.campus.housing == "No"] <- 0

# Write to csv 
write.csv(md,'merged_cases.csv')
```

We then look to add a column to the data frame that addresses the extent to which mask mandates were present in the state in which each institution is located. We read out the mask mandates data from the CDC into a data frame from the CSV file, treat the appropriate columns as factors, and convert `date` into R's `Date` type.  

```{r, eval=T, echo = FALSE}
# Read mask.mandates from CSV file
mask.mandates <- read.csv("data/U.S._State_and_Territorial_Public_Mask_Mandates_From_April_8__2020_through_August_15__2021_by_State_by_Day.csv")

# Convert appropriate columns to factors
factor.variables <- c("Face_Masks_Required_in_Public", "State_Tribe_Territory", "order_code")
mask.mandates[,factor.variables] <- lapply(mask.mandates[,factor.variables], as.factor)

# Convert column 'date' to type Date from string 
mask.mandates$date <- as.Date(mask.mandates$date, format="%m/%d/%Y")
```

The next step is to create a new simpler data frame to merge with `md`. This data frame contains only two columns: One with the name of each state, and the other with the number of days between July 1, 2020, and May 26, 2021, during which face masks were required in public in that state. We then merge this data frame with `md` to create `full.cases`, and create a column `total.cases` in `full.cases` that sums the `cases` and `cases_2021` columns.

```{r, eval=T, echo = FALSE}
# Create new data frame to merge with md
mandates.by.state <- data.frame(state=unique(mask.mandates$State_Tribe_Territory))

# Create and fill column 'mask.mandated.days'
mandates.by.state$mask.mandated.days <- 0
for (i in 1:length(unique(mandates.by.state$state))) {
  dummy <- mask.mandates$Face_Masks_Required_in_Public[mask.mandates$State_Tribe_Territory == mandates.by.state$state[i]
                                                       & mask.mandates$date >= as.Date("2020/7/1")
                                                       & mask.mandates$date <= as.Date("2021/5/26")]
  dummy[is.na(dummy)] <- "No"
  mandates.by.state$mask.mandated.days[mandates.by.state$state == mandates.by.state$state[i]] = sum(dummy == "Yes")
}

# Add state names to mandates.by.state
mandates.by.state$full_state_name <- c("Alaska", "Alabama", "Arkansas", 
                                       "American Samoa", "Arizona", 
                                       "California", "Colorado", 
                                       "District of Columbia", "Connecticut", 
                                       "Florida", "Delaware", "Georgia", "Guam", 
                                       "Iowa", "Hawaii", "Idaho", "Illinois", 
                                       "Indiana", "Kansas", "Kentucky", 
                                       "Louisiana", "Massachusetts", 
                                       "Minnesota", "Maryland", "Maine", 
                                       "Michigan", "Missouri", 
                                       "Northern Mariana Islands", 
                                       "Mississippi", "Montana", 
                                       "North Carolina", "North Dakota", 
                                       "Nebraska", "New Hampshire", "Nevada", 
                                       "New Jersey", "New Mexico", "Ohio", 
                                       "New York", "Oklahoma", "Oregon", 
                                       "South Carolina", "Rhode Island", 
                                       "Pennsylvania", "Puerto Rico", 
                                       "South Dakota", "Tennessee", "Texas", 
                                       "Utah", "Virginia", "Virgin Islands", 
                                       "Vermont", "Washington", "Wisconsin", 
                                       "West Virginia", "Wyoming")

# Drop state abbreviation column and reorder remaining two columns 
mandates.by.state <- mandates.by.state[,c(3,2)]

# Merge md and mandates.by.state data frames on name of state
full.cases <- merge(md, mandates.by.state, by.x="FIPS.state.code", 
      by.y="full_state_name")

# Create total.cases column
full.cases$total.cases <- full.cases$cases + full.cases$cases_2021

# Convert necessary columns to factors 
factor.variables <- c("religious", "FIPS.state.code", "private", 
                      "occupational.degree", 
                      "hs.equivalent.degree", "NCAA.football", 
                      "on.campus.housing", "catholic", "state")
full.cases[,factor.variables] <- lapply(full.cases[,factor.variables], as.factor)
```

Finally, we read the presidential election data from Problem Set 4 into a data frame, create a two-column data frame with the columns `state` and `gap20repub`, and merge this data frame with our data frame of observations on the `state` variable. 

```{r, eval=T}
elections.data <- read.csv("data/pres_elections.csv")
voter.gap <- elections.data[,c("state", "gap20repub")]
voter.gap$state[voter.gap$state == "DC"] <- "Washington, D.C."

full.cases <- merge(full.cases, voter.gap, by="state")
write.csv(full.cases, "full_cases.csv")
```

# Description of variables

After performing the data cleaning procedures outlined above, we are left with the following ADJUST NUMBER! columns:

* **ipeds_id** 

* **institution.name**

* **state**    

* **private**

* **religious.affiliation**

* **religious**

* **catholic**

* **tuition**

* **total.headcount**

* **percent.american.native**

* **percent.asian**

* **percent.black**

* **percent.hispanic.latino** 

* **percent.pacific.islander**

* **percent.white**     

* **percent.two.more.races**

* **percent.NA.race**

* **percent.nonres.alien**

* **percent.women** 

* **avg.grant.money**

* **grad.rate**

* **percent.fin.aid** 

* **percent.student.loan**

* **occupational.degree**

* **hs.equivalent.degree**

* **on.campus.housing** 

* **dorm.capacity**

* **city**

* **college**

* **cases**

* **cases_2021**

* **religious**   

* **catholic**

* **private**

* **mask.mandated.days**

* **total.cases**

* **gap20repub**


# Group Testing

To begin our analyses, we will perform a range of group tests to determine whether the institutions with a religious affiliation had a higher true average number of Covid cases than religious institutions without a religious affiliation. In the following analyses, we will test whether or not the true average number of cases in the fall semester — i.e., the case counts for just 2020 in the NYT data set, `cases` — were different for these two groups of institutions, as well as whether the true average number of cases for the entire academic year —  see the description of `total.cases` — different for the two groups of institutions. We will perform both sets of analyses, because they are so fast and easy to perform and interpret. Later, as we bfit linear models to perform more complicated statistical inference procedures with respect to our data set, we will demonstrate why a focus will be placed on case counts from the fall semester only (see "Determining whether data from 2020 or 2020 and 2021 should be used").

## Checking the assumptions for $t$-based methods

We begin with the most commonly used test for a difference in means: the Student-$t$ test. Of course, before we can perform this test, we must make sure that its underlying assumptions are reasonable. Because we have no reason to assume that the variances in the observations between both groups — religiously affiliated and not religiously affiliated universities — are the same, we will use the unpooled $t$-test. For unpooled $t$-based test for a difference in sample means, there are three assumptions:

1) **Observations are independent.** We claim that this observation is reasonable. Even though the geographic proximity of universities — or just a relationship between universities in which students from one frequently visit the other — might have caused there to be some correlation between case counts at these different schools, we claim that since universities are somewhat insular communities — students tend to stay within their own social sphere of their university —  this is not a major concern. Overall, with such a rich data set, in which a wide and diverse range of characteristics are expressed across all 1,855 observations, this assumption of independence of observations seems reasonable. 

2) **Groups are independent of one another.** There is no reason to suggest that the two groups in question — religious institutions and non-religious institutions — are not independent from one another. Above, we justified that all of the observations in our data set are sufficiently independent from one another — if this is true, there is no reason to suggest that these two groups of religious institutions would not be independent of one another. 

3) **Observations are normally distributed.** As is shown in the plot below (left), both of the response variables in question are *not* normally distributed; after log-transforming these responses, however, we see that their distirbution is symmetrical enough to satisfy this assumption. Of course, we do not need to seek perfect normality with respect to these distributions, since $t$-based statistical inference procedures are robust to this assumption (as was demonstrated in Question 5 in Problem Set 3).

```{r, eval=T}
epsilon <- 1

par(mfrow=c(2,2))
hist(full.cases$cases, main="2020 cases, untransformed", xlab="2020 cases")
hist(log(full.cases$cases + epsilon), main="2020 cases, log-transformed", 
     xlab="2020 cases")
hist(full.cases$total.cases, main="total.cases, untransformed", 
     xlab="total.cases")
hist(log(full.cases$total.cases + epsilon), main="total.cases, log-transformed",
     xlab="total.cases")
```

The Normal Q-Q Plots below corroborate the conclusion that this assumption is reasonable. In each plot, the data for the most part follows the line that we would expect if it were perfectly normally distributed; although, since the data sits below the line at both the positive and negative extremes of the graph, the left tail is a bit fatter and the right tail a bit skinnier than the perfectly normal distribution in each of these sample distributions. 

```{r, eval=T}
par(mfrow=c(1,2))
qqnorm(log(full.cases$cases + epsilon), main="Normal Q-Q Plot, 2020 cases")
qqline(log(full.cases$cases + epsilon))
qqnorm(log(full.cases$total.cases + epsilon), 
       main="Normal Q-Q Plot, total.cases")
qqline(log(full.cases$total.cases + epsilon))
```

## Student-$t$ Tests 

Now that we have shown the assumptions of Student-$t$ tests to be reasonable, we can proceed to perform the test itself. We perform two two-sided $t$-tests for a difference in means. In each case, the null hypothesis is that the true mean case counts in the two groups — religiously affiliated and non-religiously affiliated institutions — are equal over the given time period, and the alternative is that they are not equal. As in the rest of the paper, we will use the standard confidence level of $\alpha = 0.05$, for the sake of convenience and conformity. Below are the results from these tests. 

```{r, eval=T, render=lemon_print}
cases.t.test <- t.test(log(cases + epsilon) ~ religious, data=full.cases)
total.t.test <- t.test(log(total.cases + epsilon) ~ religious, data=full.cases)

data.frame(sample=c("2020 only", "entire year"), 
           test.statistic=c(cases.t.test$statistic, total.t.test$statistic),
           df=c(cases.t.test[[2]], total.t.test[[2]]),
           p.value=c(cases.t.test$p.value, total.t.test$p.value))
```

In both tests, we see that our p-value is greater than the significance level of 0.05; we thus fail to reject the null hypothesis in both cases — we do not have statistically significant evidence to suggest that the true mean case counts, both for the fall semester and for the entire year, are different between the religiously affiliated and non-religiously affiliated institutions. Note that the degrees of freedom — which are not integers owing to R's use of the Welch approximation — are different for the two tests. This is because the test using the `total.cases` data has fewer observations, since some schools in our sample did not report case counts for the spring semester. This issue is tackled in greater detail below (see "Determining whether data from 2020 or 2020 and 2021 should be used"). 

## Non-Parametric Testing — Wilcox Rank Sum Test

We will also perform the non-parametric Wilcox Rank Sum Test to determine whether the case counts are different between religiously affiliated and non-religiously affiliated schools. The strength of this non-parametric test lies in the fact that it does not rely on a assumption on the distribution of the data-generating process of the sample follows. Thus, though we are confident in having shown above that the distributions of the log-transformed responses are sufficiently approximately normal, for the sake of completeness, we will perform this non-parametric test, and see if it leads us to the same conclusions. 

Again, we will perform the Wilcox Rank Sum Test on data from just the fall semester and on data from the entire academic year. For the following two tests, the null hypothesis is that the true average quantiles within the two groups — when the data from both groups are ranked together — are the same, while the alternative hypothesis is that there is an association between group status and the average quantile of the observations in the entire population. 

```{r, eval=T, render=lemon_print}
cases.wilcox <- wilcox.test(x = full.cases$cases[full.cases$religious == "Yes"], 
            y = full.cases$cases[full.cases$religious == "No"], 
            alternative='two.sided', exact = FALSE, correct = FALSE, 
            conf.int = TRUE)
total.wilcox<-wilcox.test(x=full.cases$total.cases[full.cases$religious=="Yes"], 
            y = full.cases$total.cases[full.cases$religious == "No"], 
            alternative='two.sided', exact = FALSE, correct = FALSE, 
            conf.int = TRUE)

data.frame(sample=c("2020 only", "entire year"), 
           test.statistic=c(cases.wilcox$statistic, total.wilcox$statistic),
           p.value=c(cases.wilcox$p.value, total.wilcox$p.value))
```

As the results above show, neither test was statistically significant at the 0.05 confidence level; we thus again fail to reject the null hypothesis, concluding that there is no statistically significant evidence to suggest that the there is an association between religious affiliation and average quantile of case counts in either sample. 

# Basic Linear Regression Models

Of course, group tests are limited in that they do not take into account potential confounding variables that might reveal the significance of an institution's having a religious affiliation. To take into account the effects of these potential confounding variables — which we have already identified at length, as evidenced by the extensive list of predictors we compiled before beginning our analyses — we will fit linear models. We will then use these linear models to perform statistical inference on the coefficient of the `religious` predictor, determining whether there is a statistically significant relationship between an institution's religious affiliation and the number of cases that it recorded.

## Checking the Assumptions of Linear Regression Models

Before we can fit more complicated models, we must first check the assumptions of linear regression. We begin by checking the assumption of linearity. To do so, we plotted `cases` versus all of the quantitative predictors that we would like to include in our analyses (the complete list of these plots can be found in Appendix A). We the identified that `total.headcount`, `percent.american.native`, `percent.asian`, `percent.black`, `percent.hispanic.latino`, and `percent.pacific.islander` would benefit from being log-transformed; given the left-skewness of its distribution, we also determined that `percent.fin.aid` would be best transformed using the following transformation `log(100 - percent.fin.aid + 1)`. The following plots show that the distribution of these predictors before and after being transformed. *In these plots, the y-axis is always log-transformed cases* (2020); the axis label is not printed in order to save space. 

```{r, eval=T, fig.height=1.5, message=FALSE}
library(patchwork)
suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=total.headcount)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(total.headcount))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.american.native)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(percent.american.native))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.asian)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(percent.asian))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.black)) + geom_point() + ylab("")) +
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(percent.black))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.hispanic.latino)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(percent.hispanic.latino))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.pacific.islander)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(percent.pacific.islander))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.fin.aid)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(100-percent.fin.aid+1))) + geom_point() + ylab(""))))
```

We claim that with these transformations, the assumption of linearity is reasonable. 

What remains to be shown is that the assumption of homoskedasticity is reasonable. In order to check this assumption, we fit basic regression models for `cases` and `total.cases` using all of the predictors in our predictor set.

```{r, eval=T, fig.height=4}
lm6 <- lm(log(cases + epsilon) ~ religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree, data=full.cases)

plot(lm6$residuals ~ lm6$fitted.values, main="cases (2020)", xlab="fitted values",
     ylab="residuals")
abline(h=0, col="gray")
```

As is shown in the plot above, the spread of the residuals is not constant across the entire range of fitted values — thus, it is called into question whether the assumption of homoskedasticity is reasonable in this case. The behavior of this graph is interesting: The lower bound for the residuals appears to follow a negatively sloped line, and one that contains a solid number of points. ADDRESS ASK TF!

Given that the vast majority of residuals are clustered around a region with consistent spread, we believe that the assumption of homoskedasticity will be reasonable enough to accept in this case. As a check, we will compare the standard errors generated by heteroskedasticity-consistent method with those generated by the standard OLS approach: 

```{r, eval=T, render=lemon_print}
vcov.robust = vcovHC(lm6, type="HC")
data.frame(ols=round(summary(lm6)$coef[,'Std. Error'],4), 
           robust=round(sqrt(diag(vcov.robust)),4), 
           "absolute difference"=round(abs(sqrt(diag(vcov.robust)) - 
                            summary(lm6)$coef[,'Std. Error']),4))
```

As the table above shows, the standard errors are the most part consistent between the two models. ADDRESS ASK TF!

```{r, eval=T, fig.height=4}
lm7 <- lm(log(total.cases + epsilon) ~ religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree, data=full.cases)

plot(lm7$residuals ~ lm7$fitted.values, main="total.cases (2020-21)", 
     xlab="fitted values", ylab="residuals")
abline(h=0, col="gray")
```

In the above plot for the model for `total.cases`, the spread of the residuals is much more consistent across the entire range of fitted values. Though it is not entirely uniform, it would appear reasonable enough to operate under the assumption of homoskedasticity, especially given that, as was demonstrated in Problem Set 3, $t$-based statistical inference procedures done using linear models are fairly robust to slight violations in the assumption of homoskedasticity. 

## Determining whether data from 2020 or 2020 and 2021 should be used 

Naturally, there is a temptation to forget about the case data for only 2020 and to focus on case data for the entire academic year (the entire time period during which case data was collected by the *New York Times*). There is one problem, however: While the *New York Times* was able to compile case data for all of the schools in our data set for the fall semester of that year, it was not able to find data for 297 schools for the spring semester. This is a nontrivial number of observations given that our data set only consists of 1855 different institutions; thus, to determine whether it would be sound to remove these 297 observations and fit models and perform statistical inferences on data from the entire academic year, we must determine whether the two groups of schools in question — those that reported data for the entire academic year, and those that did not — are sufficiently similar to one another. 

The first step is to compare the coefficients of our baseline model (`lm6` above) when it is fit to all 1855 observations, as well as individually to the two different groups of schools in question. The coefficient estimates, along with the p-values (not adjusted to be heteroskedastically consistent) are given below.

```{r, eval=T}
lm6.both.only <- lm(formula(lm6), 
                    data=full.cases[!is.na(full.cases$cases_2021),])
lm6.twenty.only <- lm(formula(lm6),
                      data=full.cases[is.na(full.cases$cases_2021),])
modelsummary(list("all schools"=lm6, 
                  "schools with data\nfor both years"=lm6.both.only, 
                  "schools with only\n2020 data"=lm6.twenty.only),
             estimate="{estimate}", 
             statistic="{p.value}",
             shape=term ~ model + statistic)
```

As we can see, when the sample of the institutions used to fit the model changes, some of the coefficient estimates change vastly. Take, for example, the coefficient estimate for `religionYes`, which can be interpreted as the change in cases that we would expect if a given school were to have a religious affiliation rather than not have one. This coefficient estimate is not statistically significant when all institutions are considered together, is positive and statistically significant when the institutions that reported data for the entire academic year are considered, and negative and statistically significant when the institutions that only reported data for the fall are considered. In fact, if one were to inspect the table above, they would notice that the majority of predictors included in this baseline models experienced changes in coefficient estimates and statistical significance as the sample of institutions considered was changed.  

Clearly, then, there are some underlying differences between the schools that did and the schools that did not report case data for the spring semester of the 2020-21 academic year. Consider, the coefficient estimate for `religiousYes`, which represents the change in the number of `cases` that we would expect given that a school were to be religiously affiliated, with all other predictors held constant. The coefficient estimate for `religiousYes` is quite different in the two models fit to data from only one of the two subgroups, which is especially interesting for our research purposes. Indeed, searching for the differences between these subgroups in terms of `religious` and other predictors might help us to answer the questions about the association between religious affiliation and reported Covid cases that are motivating this paper. 

First, we investigate the difference in the political leanings of the states in which these schools were located, as given by `gap20repub`. To do so, we perform a $t$-test (the assumptions that allow us to do so have been explored above), with the null hypothesis being that the true means `gap20repub` are equal between the two groups of institutions, and the alternative being that the true means are different. The result of this test is shown below. 

```{r, eval=T, render=lemon_print}
only.twenty <- full.cases[is.na(full.cases$cases_2021),]
both <- full.cases[!is.na(full.cases$cases_2021),]

gap20.test <- t.test(both$gap20repub, only.twenty$gap20repub)
data.frame(predictor=c("gap20repub"), test.statistic=c(gap20.test$statistic),
           df=c(gap20.test[[2]]),
           p.value=c(gap20.test$p.value))
```

With a p-value that is much smaller than our $\alpha =0.05$ (again, because we do not assume equal variances between the groups, the Welch approximation for degrees of freedom is used), we reject the null hypothesis, concluding that the true mean `gap20repub` is different for the schools that did report case data for 2021 versus those that did not. As it turns out, the 95% $t$-based confidence interval for the mean `gap20repub` for the schools that did report 2021 case data minus the mean `gap20repub` for the schools that did not is (-9.468120, -4.959647) — thus, we conclude that the schools that did report 2021 case data are located in states that are considerably more left-leaning than the schools that did not. 

We repeat this sort of analysis for `total.headcount`, `tuition`, `percent.fin.aid`, `mask.mandated.days`, `on.campus.housing`, and `religious`, predictors that we have chosen based on large differences in the coefficient estimates produced by the models above when fit to only one of the two subgroups of institutions. We use a two-sided $t$-test for the quantitative variables and two-sided z-tests for proportions (ADDRESS) for the `categorical` predictors. The hypotheses are similar to above, the null being that the true means/proportions are equal between the two groups, and the alternative being that they are not equal. As per the results shown below, the tests for all six of these predictors showed statistically significant differences between the two groups of universities. 

```{r, eval=T, render=lemon_print}
size.test <- t.test(both$total.headcount, only.twenty$total.headcount)
tuition.test <- t.test(both$tuition, only.twenty$tuition)
fin.aid.test <- t.test(both$percent.fin.aid, only.twenty$percent.fin.aid)
mask.test <- t.test(both$mask.mandated.days, only.twenty$mask.mandated.days)
on.campus.test <- prop.test(x=matrix(c(sum(both$on.campus.housing == "Yes"), sum(both$on.campus.housing == "No"), 
                     sum(only.twenty$on.campus.housing == "Yes"), 
                     sum(only.twenty$on.campus.housing == "No")), nrow=2, byrow=T), 
          n=c(length(both$on.campus.housing), length(only.twenty$on.campus.housing)))
religious.test <- prop.test(x=matrix(c(sum(both$religious == "Yes"), sum(both$religious == "No"), 
                     sum(only.twenty$religious == "Yes"), 
                     sum(only.twenty$religious == "No")), nrow=2, byrow=T), 
          n=c(length(both$religious), length(only.twenty$religious)))

data.frame(predictor=c("total.headcount", "tuition", "percent.fin.aid",
                       "mask.mandated.days", "on.campus.housing", 
                       "religious"),
           test.statistic=c(size.test$statistic, tuition.test$statistic,
                            fin.aid.test$statistic, mask.test$statistic,
                            on.campus.test$statistic, religious.test$statistic),
           df=c(size.test[[2]], tuition.test[[2]], fin.aid.test[[2]], 
                mask.test[[2]], on.campus.test[[2]], religious.test[[2]]),
           p.value=c(size.test$p.value, tuition.test$p.value, 
                     fin.aid.test$p.value, 
                     mask.test$p.value, on.campus.test$p.value, 
                     religious.test$p.value))
```

The existence of these statistically significant differences between these two groups of institutions leads us to two distinct conclusions: Firstly, that we should perform all further analyses on data only from the fall 2020 semester, and secondly, that we should consider interaction effects in our model to help better isolate and take into account these demonstrated differences between these two different groups of institutions.  

# Linear Models with Interaction Effects

Above we performed several linear regressions with no interactions and multiple predictors. Our "base model" includes all of the predictors we include in this paper, which are: `religious`, `tuition`, `total.headcount`, `percent.american.native`, `percent.asian`, `percent.black`, `percent.hispanic.latino`, `percent.pacific.islander`, `percent.white`, `percent.two.more.races`, `percent.women`, `grad.rate`, `percent.fin.aid`, `on.campus.housing`, `gap20repub`, `private`, `percent.student.loan`, `mask.mandated.days`, `occupational.degree` and `hs.equivelant.degree`. Above we discuss using 2020 cases, 2021 cases, or the total number of cases, concluding that we will use only 2020 cases for the remaining models. 

We will first look at three linear models with all of the predictors in this paper: our "base model," an interaction model between the `religious` predictor and the others (`religiousInteraction`), and a full interaction model (`fullInteraction`). 

```{r}
lm7 <- lm(log(cases + epsilon) ~ religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree, data=full.cases)
summary(lm7)
```

Above is the base model. We see that there are a lot of statistically significant coefficients on the predictors, holding all else constant. These are noted with a star next to them in the above table. Notably, the coefficient for the `religious` variable does not carry statistical significance, with a p-value of .25190, significantly above $\alpha = 0.05$. This could be for a number of reasons, including that 1) Religious affiliation (either Yes: Religious or No: Not Religious) simply has no correlation with 2020 Covid cases or 2) The `religious` variable shares predictive power (has some degree of collinearity) with other predictor variables. 

We will determine this relationship by fitting the `religiousInteraction` model; a linear model that includes the interactions between the religious variable and the remaining confounding variables.

```{r}
religiousInteraction <- lm(log(cases + epsilon) ~ religious*( tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree), data=full.cases)

#summary(religiousInteraction)
```

The `religiousInteraction` model clarifies the significance of the `religious` predictor: When accounting for the interactions between `religious` and other predictors, as seen in this model, `religious` becomes a statistically significant predictor, with a p-value of 0.000290, significantly below the p-value of $\alpha = 0.05$. This indicates that the previous "base model" had `religious` as an insignificant predictor because it interacted with other variables which were collinear and therefore shared a lot of predictive power. Given the number of confounding variables, this result makes sense. We see in this model that the coefficient on the `religiousYes` variable is 3.665, indicating that when looking at a religiously-affiliated school versus one that is not, holding all other variables constant, the religious affiliation alone implies an increase of 3.665 Covid cases. 

`religious` had a statistically significant interactive term with 5 other variables, which were: `percent.black`, `percent.american.native`, `percent.fin.aid`, `on.campus.housing`, and `mask.mandated.days`. For example, the term `religiousYes:mask.mandated.days` in the summary results of the interaction model had a coefficient of 2.275e-03 and a p-value of 6.43e-05. This implies that it is statistically significant in this model, and for religiously affiliated schools, a one-day increase in mask mandates correlates with a very small increase in Covid cases compared to its non-religious counterpart. This result is interesting, and could prompt further investigation in a different paper. 

Given the significance of the interaction terms in our partial interaction model, we will now look at a full interaction model with all predictive variables in this paper to predict the 2020 Covid cases:

```{r}
fullInteraction <- lm(log(cases + epsilon) ~ (religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree)^2, data=full.cases)

#summary(fullInteraction)
```
The `religious` variable in the above model with all interactive terms is not statistically significant in predicting 2020 Covid cases at Universities, with a p-value of 0.290212. But, very few terms alone are significant in the model; only `percent.black`, `percent.two.more.races`, and `on.campus.housing`. Even the predictor `total.headcount` is not statistically significant in this model, whereas it is consistently very significant in other models given the obvious relationship between the total number of people on a given campus and the number of Covid cases amongst them. This result suggests that a lot of the variables are correlated with one another to varying degrees. Thus, there are many interactions, and therefore predictive terms in this model. These have taken the predictive power from the `religious` variable by itself; though some interactions with the `religious` term are statistically significant, such as `percent.asian`, `percent.black`, `mask.mandated.days`, and `on.campus.housing`. Ultimately a high number of predictive terms in a model dealing with highly collinear data will inherently diminish the predictive power from a term that shares a lot of correlation with others (referencing `religious`, amongst other terms). With so many predictors, the $t$-tests used to determine the statistical significance of each coefficient estimate are also not very powerful, as the number of degrees of freedom is quite high. 

```{r}
# MAKE THIS INTO A TABLE ?
# I like that idea — Daniel 
summary(lm7)$r.squared
summary(religiousInteraction)$r.squared
summary(fullInteraction)$r.squared
```

We see in the above table that the $R^2$ value for the three models increases as the number of interactive terms increases. This indicates the significance of the interactive terms in the prediction model. It is unsurprising that the greatest increase is between the `religiousInteraction` model and the `fullInteraction` model. This can be explained by the sheer number of interactive terms in the model, as well as the collinearity between a lot of variables in the data. 

We now perform ESS $F$-tests to compare the predictive power the base model to `religiousInteraction` and `religiousInteraction` to `fullInteraction`. In each test, the null hypothesis is that the true coefficients of the additional predictors included in the larger model — `religiousInteraction` in the first test, and `fullInteraction` in the second — are all zero; the alternative hypothesis is that the true coefficient of one or more of these predictors is not zero, i.e., that the additional predictors provide explanatory power. The result of these ESS $F$-tests are shown below. 

```{r}
# ESS F-test 1
f.test.1 <- anova(lm7, religiousInteraction)

# ESS F-test 2
f.test.2 <- anova(religiousInteraction, fullInteraction)

data.frame(ESS.F.test=c("base model vs. religiousInteraction", 
                  "religiousInteraction vs. fullInteraction"), 
           F.statistic=c(f.test.1$'F'[2], f.test.2$'F'[2]),
           df=c(f.test.1$Df[2], f.test.2$Df[2]),
           p.value=c(f.test.1$`Pr(>F)`[2], f.test.2$`Pr(>F)`[2]))

# UNSURE OF WHAT THESE RESULTS MEAN given that most are very small numbers. 
#vcov7 = vcov(lm7)
#vcov7[2,]
```

In the first test, because the p-values of each test are below our $\alpha=0.05$, we reject the null hypothesis, and conclude that there is evidence that the `religious` interaction terms contribute to the model. Furthermore, in the second ESS F-test between the `religiousInteraction` model and the `fullInteraction` model, we again reject the null hypothesis, concluding that there is evidence that the additional interaction terms in `fullInteraction` have some explanatory power. This indicates that though `religious` has some significant interaction with the confounding variables (as discussed above), these predictors interact with one another more significantly due to high collinearity in the data. HMM, I'M NOT SURE I UNDERSTAND THIS CONCLUSION

We used the above variance-covariance matrix result for the covariance values between `religious` and all other variables to create a simplified linear regression model with the most correlated confounding variables removed. The refined full interaction model is below, with the terms `private`, `percent.pacific.islander`, `percent.hispanic.latino`, `on.campus.housing`, `percent.native.american`, `gap20repub`, `percent.women`, `grad.rate`, `percent.white`, `percent.student.load`, and `mask.mandated.days` removed. 

```{r}

simple.fullInteraction <- lm(log(cases + epsilon) ~ (religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + 
             percent.two.more.races + 
              log(100-percent.fin.aid+epsilon) + 
            occupational.degree + hs.equivalent.degree)^2, data=full.cases)

summary(simple.fullInteraction)
```
We see above that we have artificially made the `religious` predictor significant in the full interactive model by removing terms that it is collinear with. But, this comes at the expense of the $R^2$ value, which has decreased even from the "base model" with all predictors but no interactions. This indicates that the variables we removed are important in the prediction of 2020 Covid cases. 


```{r}

# Not sure if to include; RMSEs are pretty similar and I am getting errors on the interaction models. 
# The errors are coming from overfitting, I think —Daniel 

RMSE <- function(y, yhat) {
     SSE = sum(na.omit(y-yhat)^2)
     return(sqrt(SSE/length(y)))
}

# getting errors so maybe don't use?
RMSE(full.cases$cases, predict(lm7, full.cases))
RMSE(full.cases$cases, predict(religiousInteraction, full.cases))
RMSE(full.cases$cases, predict(simple.fullInteraction, full.cases))

```

# In Search of a Parsimonious Model: Sequential Variable Selection Models

Of course, both `religiousInteraction` and `fullInteraction` are large, complicated models with many predictors — this makes them hard to interpret, and also leaves our significance tests diminished in statistical power. In order to find more parsimonious models — i.e., remove some predictors from these models — we performed Sequential Variable Selection using AIC as our model comparison criteria, stepping backward, forward, and in both directions. Brief descriptions and results from each resulting model are below. 

- Backward step model: started with the "base model" with no interactive terms and all variables in this paper. This removed variables `religious`, `percent.american.native`, `percent.hispanic.latino`, `percent.pacific.islander`, `mask.mandated.days`. 

- Forward step: started with the "base model" with no interactive terms and all variables. Upper model chosen was the full interactive model, which resulted in the addition of a number of variables. Note: also looked at using the backward step model as the base for a forward step model, as well as the intercept model... only the first included religious because it had to?

- Combined step: lower model set as the intercept model and upper model set as the full two-way interaction model. Resulted in keeping `religious` as a predictor, amongst others. 

Backward stepwise is not the best for our circumstance because of collinearity; backward stepwise might be forced to keep collinear variables because it starts with them, whereas forward stepwise can entirely omit them from the procedure. 

Combined step has better AIC. 

```{r, eval=T}
# A more convenient data frame to work with for future models 
df <- full.cases[,c("gap20repub", "mask.mandated.days", "private","tuition","total.headcount","percent.american.native","percent.asian","percent.black","percent.hispanic.latino","percent.pacific.islander","percent.white","percent.two.more.races","percent.women","grad.rate","percent.fin.aid","percent.student.loan","on.campus.housing","religious","cases", "occupational.degree", "hs.equivalent.degree")] 
df <- na.omit(df) # this drops 150 rows
```

```{r, eval=T, cache=T}
back.step <- step(fullInteraction, direction = "backward", k=2, trace=0)

model0 <- lm(log(cases+epsilon) ~ 1, df)

forward.step <- step(lm7, scope = list(upper = formula(fullInteraction)), 
                     direction = "forward", k=2, trace=0)
forward.step2 <- step(back.step, scope = list(upper = formula(fullInteraction)), 
                      direction = "forward", k=2, trace=0)
forward.step1 <- step(model0, scope = list(upper = formula(fullInteraction)), 
                      direction = "forward", k=2, trace=0)

both.step = step(lm7, scope = list(lower = formula(model0), upper = formula(fullInteraction)),
            direction = "both", k=2, trace=0)
```

```{r, eval=T}
step.models <- list(back.step, forward.step, forward.step1, forward.step2, both.step)
lapply(step.models, FUN=AIC)
lapply(step.models, FUN=formula)
```

See above "analysis" ... not sure what else to say. Combined step includes religious, the others do not. 

COMMENT: I THINK MAYBE GIVING THE FORMULAS FOR EACH MODEL WOULD BE USEFUL FOR REFERENCE. WE ALSO SHOULD TRY TO INCLUDE SOME INFERENCE IN THIS SECTION — IN THESE MODELS, IS `religious` STATISTICALLY SIGNIFICANT? 


# LASSO for Variable Section

Because sequential variable selection can be sensitive to outliers and high-leverage observations, and in order to get a better idea of which of the three sequential variable selection models might be the most reliable, we will employ an alternative to this method: LASSO. A penalized form of regression, LASSO is able to function as a method of variable selection, since, unlike ridge regression, it actually does shrink coefficient estimates to exactly zero. 

We will apply LASSO to both the `religiousInteraction` and `fullInteraction` models, and interpret the results of each model. In each case, we will use cross-validation to select the best regularizing constant $\lambda$ with which to fit each model. 

First, we do so for the `religiousInteraction` model. We plot the $\lambda$ values that we cross validated to make sure that the minimum value given by R is not the lower or upper bound of those considered (which would suggest that we widen our range of $\lambda$ values considered); we then fit a LASSO model using the formula from `religiousInteraction` — call this model LASSO Model 1 — and print the list of coefficients whose estimates were shrunk all the way to zero. 

```{r, eval=T, fig.height=3}
lambda.range <- 10^seq(-10, 3, 0.1)
rI.cv <- cv.glmnet(model.matrix(religiousInteraction), df$cases, 
                   lambda=lambda.range, alpha=1, trace=0)
plot(rI.cv)
rI.lasso <- glmnet(model.matrix(religiousInteraction), df$cases,
                   lambda=rI.cv$lambda.min, alpha=1, trace=0)
rI.lasso.coef <- data.frame(coefficient=colnames(model.matrix(religiousInteraction)), beta=matrix(rI.lasso$beta)[,1])
rI.lasso.coef$coefficient[rI.lasso.coef$beta == 0]

rI.lasso.2 <- glmnet(model.matrix(religiousInteraction), df$cases,
                   lambda=lambda.range, alpha=1, trace=0)
```

Clearly, the list is short — most of the predictors were not shrunk all the way to zero. We now perform the same procedure with the `fullInteraction` model, creating LASSO Model 2: 

```{r, eval=T}
fI.cv <- cv.glmnet(model.matrix(fullInteraction), df$cases, 
                   lambda=lambda.range, alpha=1, trace=0)
plot(fI.cv)
fI.lasso <- glmnet(model.matrix(fullInteraction), df$cases,
                   lambda=fI.cv$lambda.min, alpha=1, trace=0)
fI.lasso.coef <- data.frame(coefficient=colnames(model.matrix(fullInteraction)), beta=matrix(fI.lasso$beta)[,1])
fI.lasso.coef$coefficient[rI.lasso.coef$beta == 0]
```

This time around, there are 11 coefficient estimates that were shrunk all the way to zero. 

In order to determine which of the remaining predictors in each model are statistically significant, we refit a linear model using all the coefficients that were not shrunk to zero in each of LASSO Models 1 and 2. Note that even though the intercept was shrunk to zero by LASSO in both cases, we still include an intercept in this new model to make interpretation of the coefficient estimates in the resulting models easier. Call the new linear models LASSO-Selected Linear Model 1 and LASSO-Selected Linear Model 2. 

```{r, eval=T}
rI.lasso.remaining <- lm(log(cases + epsilon) ~ religious*(tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree) -
              religious:percent.women, data=full.cases)
rI.stat.sig <- names(summary(rI.lasso.remaining)$coefficients[summary(rI.lasso.remaining)$coefficients[,"Pr(>|t|)"] < 0.05,1])

fI.lasso.remaining <- lm(log(cases + epsilon) ~ (religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree)^2 - 
              religious:percent.women - tuition:log(total.headcount + epsilon) -
              log(total.headcount + epsilon):private - 
              log(percent.american.native + epsilon):percent.two.more.races -
              log(percent.black + epsilon):percent.women - 
              log(percent.hispanic.latino + epsilon):log(percent.pacific.islander +
                                                           epsilon) -
              percent.white:gap20repub - percent.two.more.races:gap20repub -
              on.campus.housing:gap20repub - gap20repub:hs.equivalent.degree,
            data=full.cases)
fI.stat.sig <- names(summary(fI.lasso.remaining)$coefficients[summary(fI.lasso.remaining)$coefficients[,"Pr(>|t|)"] < 0.05,1])
```

In LASSO-Selected Linear Model 1, we found the following coefficient estimates to be statistically significant:
`(Intercept)`, `religiousYes`, `tuition`,`log(total.headcount + epsilon)`, `log(percent.black + epsilon)`, `log(percent.pacific.islander + epsilon)`, `percent.white`, `percent.women`, `grad.rate`, `on.campus.housingYes`, `gap20repub`, `privateYes`, `percent.student.loan`, `religiousYes:log(percent.american.native + epsilon)`, `religiousYes:log(percent.black + epsilon)`, `religiousYes:grad.rate`, `religiousYes:on.campus.housingYes`, `religiousYes:mask.mandated.days`. 

In LASSO-Selected Linear Model 2, we found the following coefficient estimates to be statistically significant: `religiousYes`, `log(percent.black + epsilon)`, `on.campus.housingYes`, `religiousYes:mask.mandated.days`, `tuition:percent.two.more.races`, `tuition:grad.rate`, `tuition:percent.student.loan`, `log(total.headcount + epsilon):log(percent.black + epsilon)`, `log(total.headcount + epsilon):grad.rate`, `log(percent.american.native + epsilon):gap20repub`, `log(percent.american.native + epsilon):privateYes`, `log(percent.asian + epsilon):log(percent.pacific.islander + epsilon)`, `log(percent.asian + epsilon):gap20repub`, `log(percent.black + epsilon):gap20repub`, `log(percent.hispanic.latino + epsilon):percent.white`, `log(percent.hispanic.latino + epsilon):grad.rate`, `percent.white:on.campus.housingYes`, `percent.two.more.races:mask.mandated.days`, `percent.two.more.races:occupational.degreeYes`, `percent.women:log(100 - percent.fin.aid + epsilon)`, `percent.women:on.campus.housingYes`, `percent.women:gap20repub`, `percent.women:percent.student.loan`, `grad.rate:log(100 - percent.fin.aid + epsilon)`, `grad.rate:gap20repub`, `grad.rate:hs.equivalent.degreeYes`, `on.campus.housingYes:percent.student.loan`, `on.campus.housingYes:mask.mandated.days`, `on.campus.housingYes:occupational.degreeYes`, `on.campus.housingYes:hs.equivalent.degreeYes`, `percent.student.loan:hs.equivalent.degreeYes`.

In both LASSO-Selected Linear Models 1 and 2, we find that `religiousYes` is statistically significant. Recall that this is the categorical indicator variable for `religious`, which can be interpreted as the change in cases that we would expect for an institution with a given predictor set if it were to have a religious affiliation, rather than not have one, and all the other predictors were to remain constant.

Interestingly, while five different `religious` interaction coefficients are statistically significant in LASSO-Selected Linear Model 1, only one `religious` interaction coefficient is statistically significant in LASSO-Selected Linear Model 2: `religiousYes:mask.mandated.days`. Thus, while the first model suggests that the relationship between `religious` and `cases` is affected by hte value of five other predictors in the model, the second suggests that the association between `religious` and `cases` changes based on the value of `mask.mandated.days` of a given observation. It appears that the interaction effects deemed to be statistically significant in LASSO-Selected Linear Model 1 shared some level of collinearity with some of the additional interaction effects that were included in LASSO-Selected Linear Model 2. In this way, when these other interaction effects were included in the model, the `religious` interaction effects were no longer statistically significant, as the relationships that they helped to describe were better captured by other predictors. 

The plot below demonstrates that there does indeed exist collinearity between the predictors of LASSO Model 2 (and thus LASSO-Selected Model 2). We have chosen to present this plot, because it is more readable than a printout of a massive variance-covariance matrix. This plot shows the trajectories of the coefficient estimates of LASSO Model 2 as the regularizing constant $\lambda$ increases on a logarithmic scale. As is clear, not all of the coefficient estimates are shrunk uniformly towards zero; instead, some display sharp increases in magnitude as $\lambda$ increases. This demonstrates that there is collinearity between some of the predictors: As one predictor in a collinear pair is shrunk to zero, the magnitude of the coefficient estimate for the other increases in order to make up for the lost predictive power. 

```{r, eval=T, fig.height=4}
fI.lasso.2 <- glmnet(model.matrix(fullInteraction), df$cases, 
                   lambda=lambda.range, alpha=1, trace=0)
betas <- fI.lasso.2$beta
log_lam <- log(fI.lasso.2$lambda)
plot(betas[1,] ~ log_lam, type="l", ylab=expression(beta), 
     xlab=expression(log(lambda)), ylim=c(-150,150), xlim=c(-7, max(log_lam)),
     main="LASSO Model 2: Coefficient Estimate Trajectories")
for (i in 2:dim(betas)[1]) {
  lines(betas[i,]~log_lam, lty=i, col=i)
}
```

In the end, we lean towards accepting the conclusions presented by the second model: With a wider range of interaction effects to consider, it is less likely that the statistically significant predictors were only marked as such because they help explain a relationship between `cases` and another predictor with which they are collinear. 

LASTLY, COMPARE LASSO RESULTS TO SEQUENTIAL VARIABLE SELECTION RESULTS

# Hierarchical Multi-level Models

# Conclusions


# Appendix A: Plots to Check the Assumption of Linearity

```{r, eval=T}
par(mfrow=c(3,2), mar=1.5 * c(1,1,1,1), cex=0.5)
plot(log(total.cases + epsilon) ~ tuition, data=full.cases,
     main="tuition")
plot(log(total.cases + epsilon) ~ total.headcount, data=full.cases,
     main="total.headcount")
plot(log(total.cases + epsilon) ~ percent.american.native, data=full.cases,
     main="percent.american.native")
plot(log(total.cases + epsilon) ~ percent.asian, data=full.cases)
plot(log(total.cases + epsilon) ~ percent.black, data=full.cases)
plot(log(total.cases + epsilon) ~ percent.hispanic.latino, data=full.cases)
par(mfrow=c(3,2), mar=2 * c(1,1,1,1), cex=0.5)
plot(log(total.cases + epsilon) ~ percent.pacific.islander, data=full.cases)
plot(log(total.cases + epsilon) ~ percent.white, data=full.cases)
plot(log(total.cases + epsilon) ~ percent.two.more.races, data=full.cases)
plot(log(total.cases + epsilon) ~ percent.women, data=full.cases)
plot(log(total.cases + epsilon) ~ grad.rate, data=full.cases)
plot(log(total.cases + epsilon) ~ percent.fin.aid, data=full.cases)
par(mfrow=c(3,2), mar=2 * c(1,1,1,1), cex=0.5)
plot(log(total.cases + epsilon) ~ gap20repub, data=full.cases)
plot(log(total.cases + epsilon) ~ percent.student.loan, data=full.cases)
plot(log(total.cases + epsilon) ~ mask.mandated.days, data=full.cases)
```

