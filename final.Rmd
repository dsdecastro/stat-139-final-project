---
title: "Characterizing the relationship between the religious affiliation and incidences of Covid-19 at U.S. universities."
author: "Daniel de Castro and Laura Appleby"
date: "December 14, 2022"
output: pdf_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, results=TRUE, messages=FALSE)
```

```{r, eval=T, include=F}
library(dplyr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(modelsummary)
library(sandwich)
library(ggplot2)
library(lme4)
library(sjPlot)

library(lemon)
knit_print.data.frame <- lemon_print

library(glmnet)
```

```{r, eval=T, include=F}
RMSE <- function(y, yhat) {
  SSE = sum((y-yhat)^2)
  return(sqrt(SSE/length(y)))
}
```

# Introduction

Although Covid-19 pandemic may feel like a thing of the past, there is still much research to do in order to understand its nascence, spread, and lasting health effects. Indeed, we, as a society, must be prepared to handle the next highly-transmissible pathogen that may arise. As university students whose first two years in school were far from normal on account of the Covid-19 pandemic — whose lives were dominated by case counts and triweekly testing — we are interested in exploring the development of the pandemic at colleges and universities across the United States. In particular, we are interested in the differences in how different academic institutions were affected by the pandemic. 

It has been well established that, at least in the United States, responses to the Covid-19 pandemic were tightly bound to political and ideological questions, the responses to which differ across the administrations of the wide range of universities in this country. In this paper, we will explore the differences in the impact of the pandemic between institutions with religious affiliations and those without religious affiliations. We will attempt to characterize the relationship between case counts during the fall and spring semesters of the 2020-21 academic year — the academic year in which responses to the pandemic were perhaps most varied across universities — and whether a university has a religious affiliation. We not only hope to answer the question of whether religious affiliation has a statistically significant relationship with case counts during the 2020-21 academic year — we also hope to discover the other characteristics of universities may have an effect on this relationship. To do so, we will employ a range of statistical inference procedures, from group testing to hierarchical multilevel models. 

# Description of data and source

Our data for this project comes from three sources: 

1. [NYTimes Covid-19 Data](https://github.com/nytimes/covid-19-data/tree/master/colleges). This data set is publicly available on GitHub and was the source for various New York Times maps and visuals in its coverage of the pandemic. It includes case counts from July 2020 - May 2021, and we have specifically selected cases at Universities. This dataset has 1948 entries and includes 2020 cases, 2021 cases, University IPEDS ID, University Name, State, etc. 

2. [IPEDS Data Center](https://nces.ed.gov/ipeds/use-the-data). This is a publicly available data set on colleges and universities across the globe. It has many possible variables including demographics, admission rates, religious affiliation, etc. The IPEDS data center allowed us to select which variables to include in our data set. The smallest subset of universities that included all from the NYTimes database (by IPEDS ID) was 6125 rows, with all U.S. universities. 

3. [Centers for Disease Control](https://data.cdc.gov/Policy-Surveillance/U-S-State-and-Territorial-Public-Mask-Mandates-Fro/tzyy-aayg). This publicly available data set tracks mask mandates in each state from April 8, 2020, to August 15, 2021. 

4. The presidential elections data from STAT 139 Problem Set 4. Of this data set, we will be particularly focused on the `gap20repub` predictor, which we will use to help characterize the political climate of the state in which each institution is located.  

After removing universities that were missing lots of data in the pull from the IPEDS data center, we were left with $n = 1,705$ observations.

# Data Cleaning Procedures

Our first step in cleaning the data is to read our data from CSV files into R data frames. The `colleges` data frame stores the NYT data on Covid cases at universities, while the `ipeds` data frame stores the data will most of our predictor variables (university characteristics) taken from IPEDS. We then rename most of the columns in `ipeds` to make them shorter and easier to work with. 

```{r, eval=T, echo = FALSE}
# Read colleges from csv
colleges <- read.csv("data/colleges.csv")[,-c(1,9)]

# Read ipeds from csv, drop unnecessary columns
ipeds <- read.csv("data/ipeds.csv")
ipeds <- ipeds[-c(3, 8, 22)]

# Rename columns of ipeds 
ipeds <- ipeds %>% rename_at('IC2021.Institutional.control.or.affiliation', 
                             ~'control') %>% 
                    rename_at('HD2021.FIPS.state.code', ~'FIPS.state.code') %>% 
                    rename_at('unitid', ~'ipeds_id') %>% 
                    rename_at('IC2021.Religious.affiliation', 
                              ~'religious.affiliation') %>%
                    rename_at('DRVIC2021.Tuition.and.fees..2020.21', 
                              ~'tuition') %>% 
                    rename_at('DRVEF122021.Total.12.month.unduplicated.headcount', 
                              ~'total.headcount') %>% 
                    rename_at('DRVEF122021.Undergraduate.12.month.unduplicated.headcount',
                    ~'undergrad.headcount') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.American.Indian.or.Alaska.Native', 
                              ~'percent.american.native') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.Asian', 
                              ~'percent.asian') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.Black.or.African.American', 
                              ~'percent.black') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.Hispanic.Latino', ~'percent.hispanic.latino') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.Native.Hawaiian.or.Other.Pacific.Islander', 
                              ~'percent.pacific.islander') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.White', 
                              ~'percent.white') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.two.or.more.races', 
                              ~'percent.two.more.races') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.race.ethnicity.unknown', ~'percent.NA.race') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.Nonresident.Alien',
                              ~'percent.nonres.alien') %>% 
                    rename_at('DRVEF122021.Percent.of.12.month.unduplicated.headcount.that.are.women', ~'percent.women') %>% 
                    rename_at('SFA2021.Average.amount.of.grant.and.scholarship.aid.awarded..2020.21', ~'avg.grant.money') %>% 
                    rename_at('DRVGR2021.Graduation.rate..total.cohort', ~'grad.rate') %>% 
                    rename_at('SFA2021.Percent.of.full.time.first.time.undergraduates.awarded.any.financial.aid', 
                              ~'percent.fin.aid') %>% 
                    rename_at('SFA2021.Percent.of.full.time.first.time.undergraduates.awarded.student.loans', 
                              ~'percent.student.loan') %>% 
                    rename_at('IC2021.Occupational', ~'occupational.degree') %>% 
                    rename_at('IC2021.Academic', ~'academic.degree') %>% 
                    rename_at('IC2021.Adult.basic.remedial.or.high.school.equivalent', 
                              ~'hs.equivalent.degree') %>% 
                    rename_at('IC2021.Percent.of.undergraduates..who.are.formally.registered.as.students.with.disabilities..when.percentage.is.more.than.3.percent', 
                              ~'percent.disability') %>% 
                    rename_at('IC2021.NCAA.NAIA.conference.number.football', 
                              ~'NCAA.football') %>% 
                    rename_at('IC2021.Institution.provide.on.campus.housing', 
                              ~'on.campus.housing') %>% 
                    rename_at('IC2021.Total.dormitory.capacity', 
                              ~'dorm.capacity') %>% 
                    rename_at('IC2021.Typical.room.charge.for.academic.year', 
                              ~'dorm.room.price')
```

Next, we merge the `colleges` and `ipeds` data frames on the `ipeds_id` column and remove institutions with no IPEDS data. We then create the `religious` and `private` columns, which are simply indicators for whether an institution has any religious affiliation, whether it has a catholic affiliation, and whether it is a private university. Finally, we drop any unnecessary or redundant columns from the data frame.

```{r, eval=T, echo = FALSE}
# Merge data frames, and remove duplicated "state" column 
md <- merge(ipeds, colleges, by="ipeds_id")[,-32]

# Remove institutions with no ipeds data (a lot of NAs)
md <- md[!(md$control == ""),]

# Create column 'religious' 
md$religious <- "Yes"
md[md$religious.affiliation == "Not applicable", ]$religious <- "No"

# Create column 'catholic'
md$catholic <- "No"
md[md$religious.affiliation == "Roman Catholic",]$catholic <- "Yes"

# Create column 'private' 
md$private <- "No"
md$private[md$control == "Private not-for-profit (no religious affiliation)" | 
             md$control == "Private not-for-profit (religious affiliation)"] <- 
  "Yes"

# Drop column 'control', 'academic.degree', 'dorm.room.price'
md <- md[,-c(4,24,30)]

# Set 'dorm.capacity' to 0 if no on.campus.housing
md$dorm.capacity[md$on.campus.housing == "No"] <- 0

# Write to csv 
write.csv(md,'merged_cases.csv')
```

We then look to add a column to the data frame that addresses the extent to which mask mandates were present in the state in which each institution is located. We read out the mask mandates data from the CDC into a data frame from the CSV file, treat the appropriate columns as factors, and convert `date` into R's `Date` type.  

```{r, eval=T, echo = FALSE}
# Read mask.mandates from CSV file
mask.mandates <- read.csv("data/U.S._State_and_Territorial_Public_Mask_Mandates_From_April_8__2020_through_August_15__2021_by_State_by_Day.csv")

# Convert appropriate columns to factors
factor.variables <- c("Face_Masks_Required_in_Public", "State_Tribe_Territory", "order_code")
mask.mandates[,factor.variables] <- lapply(mask.mandates[,factor.variables], as.factor)

# Convert column 'date' to type Date from string 
mask.mandates$date <- as.Date(mask.mandates$date, format="%m/%d/%Y")
```

The next step is to create a new simpler data frame to merge with the product of the last manipulations. This data frame contains only two columns: One with the name of each state, and the other with the number of days between July 1, 2020, and May 26, 2021, during which face masks were required in public in that state. We then merge this data frame with our previous data frame to create `full.cases`, and create a column `total.cases` in `full.cases` that sums the `cases` and `cases_2021` columns.

```{r, eval=T, echo = FALSE}
# Create new data frame to merge with md
mandates.by.state <- data.frame(state=unique(mask.mandates$State_Tribe_Territory))

# Create and fill column 'mask.mandated.days'
mandates.by.state$mask.mandated.days <- 0
for (i in 1:length(unique(mandates.by.state$state))) {
  dummy <- mask.mandates$Face_Masks_Required_in_Public[mask.mandates$State_Tribe_Territory == mandates.by.state$state[i]
                                                       & mask.mandates$date >= as.Date("2020/7/1")
                                                       & mask.mandates$date <= as.Date("2021/5/26")]
  dummy[is.na(dummy)] <- "No"
  mandates.by.state$mask.mandated.days[mandates.by.state$state == mandates.by.state$state[i]] = sum(dummy == "Yes")
}

# Add state names to mandates.by.state
mandates.by.state$full_state_name <- c("Alaska", "Alabama", "Arkansas", 
                                       "American Samoa", "Arizona", 
                                       "California", "Colorado", 
                                       "District of Columbia", "Connecticut", 
                                       "Florida", "Delaware", "Georgia", "Guam", 
                                       "Iowa", "Hawaii", "Idaho", "Illinois", 
                                       "Indiana", "Kansas", "Kentucky", 
                                       "Louisiana", "Massachusetts", 
                                       "Minnesota", "Maryland", "Maine", 
                                       "Michigan", "Missouri", 
                                       "Northern Mariana Islands", 
                                       "Mississippi", "Montana", 
                                       "North Carolina", "North Dakota", 
                                       "Nebraska", "New Hampshire", "Nevada", 
                                       "New Jersey", "New Mexico", "Ohio", 
                                       "New York", "Oklahoma", "Oregon", 
                                       "South Carolina", "Rhode Island", 
                                       "Pennsylvania", "Puerto Rico", 
                                       "South Dakota", "Tennessee", "Texas", 
                                       "Utah", "Virginia", "Virgin Islands", 
                                       "Vermont", "Washington", "Wisconsin", 
                                       "West Virginia", "Wyoming")

# Drop state abbreviation column and reorder remaining two columns 
mandates.by.state <- mandates.by.state[,c(3,2)]

# Merge md and mandates.by.state data frames on name of state
full.cases <- merge(md, mandates.by.state, by.x="FIPS.state.code", 
      by.y="full_state_name")

# Create total.cases column
full.cases$total.cases <- full.cases$cases + full.cases$cases_2021

# Convert necessary columns to factors 
factor.variables <- c("religious", "FIPS.state.code", "private", 
                      "occupational.degree", 
                      "hs.equivalent.degree", "NCAA.football", 
                      "on.campus.housing", "catholic", "state")
full.cases[,factor.variables] <- lapply(full.cases[,factor.variables], as.factor)
```

Finally, we read the presidential election data from Problem Set 4 into a data frame, create a two-column data frame with the columns `state` and `gap20repub`, and merge this data frame with our data frame of observations on the `state` variable. 

```{r, eval=T}
elections.data <- read.csv("data/pres_elections.csv")
voter.gap <- elections.data[,c("state", "gap20repub")]
voter.gap$state[voter.gap$state == "DC"] <- "Washington, D.C."

full.cases <- merge(full.cases, voter.gap, by="state")
write.csv(full.cases, "full_cases.csv")
```

# Description of variables

After performing the data cleaning procedures outlined above, we are left with the following predictors:

* **ipeds_id** This is the IPEDS id, specific to each institution and a commonly used identifier for Universities, created by IPEDS. 

* **institution.name** This is the institution name, as filed by each University with IPEDS.

* **state** The state the university is in. This variable has state titles, not abbreviations or codes.    

* **private** "A classification of whether an institution is operated by publicly elected or appointed officials or by privately elected or appointed officials and derives its major source of funds from private sources" [IPEDS]. This is simplified to an indicator: "Yes" for private, "No" for a public school.

* **religious** Indicator with "Yes" if the institution is religious, "No" if not. 

* **tuition** "Published tuition and fees, 2020-21 for academic year reporters only" [IPEDS].

* **total.headcount** "12-month unduplicated headcount: 2020-21" [IPEDS].

We elected to include the following demographic information to account for established inequalities in wealth and access to healthcare in the United States. 

* **percent.american.native** "Percent of 12-month unduplicated headcount that are American Indian or Alaska Native" [IPEDS].

* **percent.asian** "Percent of 12-month unduplicated headcount that are Asian" [IPEDS].

* **percent.black** "Percent of 12-month unduplicated headcount that are Black" [IPEDS].

* **percent.hispanic.latino** "Percent of 12-month unduplicated headcount that are Hispanic or Latino" [IPEDS]. 

* **percent.pacific.islander** "Percent of 12-month unduplicated headcount that are Native Hawaiian or Other Pacific Islander" [IPEDS].

* **percent.white** "Percent of 12-month unduplicated headcount that are White" [IPEDS].    

* **percent.two.more.races** "Percent of 12-month unduplicated headcount that are two or more races" [IPEDS].

* **percent.NA.race** "Percent of 12-month unduplicated headcount that are race/ethnicity unknown" [IPEDS].

* **percent.women** "Percent of 12-month unduplicated headcount that are women" [IPEDS].

The following predictors were included to better characterize each institution, including the opportunities available to and the general level of affluence of the student body, as these factors may be related to access to healthcare or attitudes towards the Covid-19 pandemic. 

* **grad.rate** "Graduation rate of first-time, full-time degree or certificate-seeking students" [IPEDS]. Included in our analysis to better characterize each institution, its student body, and its 

* **percent.fin.aid** "Percent of full-time first-time undergraduates awarded any financial aid" [IPEDS].

* **percent.student.loan** "Percent of full-time first-time undergraduates awarded student loans" [IPEDS].

* **occupational.degree** Indicator for if an occupational certificate, degree, or other formal award are offered at the institution.  

* **hs.equivalent.degree** Indicator for if a high0-school-equivalent certificate, degree, or other formal award are offered at the institution.  

Other included predictors: 

* **on.campus.housing** Indicator for if on-campus housing is offered at the institution.  

* **dorm.capacity** "The maximum number of students that the institution can provide residential facilities for, whether on or off campus. (off-campus dormitory space that is reserved by the institution)" [IPEDS].

* **college** See `institution.name`.

* **cases** 

* **cases_2021**

* **mask.mandated.days** 

* **total.cases** 

* **gap20repub** 


# Group Testing

To begin our analyses, we will perform a range of group tests to determine whether the institutions with a religious affiliation had a higher true average number of Covid cases than religious institutions without a religious affiliation. In the following analyses, we will test whether or not the true average number of cases in the fall semester — i.e., the case counts for just 2020 in the NYT data set, `cases` — were different for these two groups of institutions, as well as whether the true average number of cases for the entire academic year —  see the description of `total.cases` — different for the two groups of institutions. We will perform both sets of analyses, because they are so fast and easy to perform and interpret. Later, as we fit linear models to perform more complicated statistical inference procedures with respect to our data set, we will demonstrate why a focus will be placed on case counts from the fall semester only (see "Determining whether data from 2020 or 2020 and 2021 should be used").

## Checking the assumptions for $t$-based methods

We begin with the most commonly used test for a difference in means: the Student-$t$ test. Of course, before we can perform this test, we must make sure that its underlying assumptions are reasonable. Because we have no reason to assume that the variances in the observations between both groups — religiously affiliated and not religiously affiliated universities — are the same, we will use the unpooled $t$-test. For unpooled $t$-based test for a difference in sample means, there are three assumptions:

1) **Observations are independent.** We claim that this observation is reasonable. Even though the geographic proximity of universities might have caused there to be some correlation between case counts at these different schools, we claim that since universities are somewhat insular communities — students tend to stay within their own social sphere of their university —  this is not a major concern. Overall, with such a rich data set, in which a wide and diverse range of characteristics are expressed across all 1,705 observations, this assumption of independence of observations seems reasonable. 

2) **Groups are independent of one another.** There is no reason to suggest that the two groups in question — religious institutions and non-religious institutions — are not independent from one another. Above, we justified that all of the observations in our data set are sufficiently independent from one another — if this is true, then it is reasonable to assume that these two groups of institutions are independent of one another. 

3) **Observations are normally distributed.** As is shown in the plot below (left), both of the response variables in question are *not* normally distributed; after log-transforming these responses, however, we see that their distributions are symmetrical enough to satisfy this assumption. Of course, we do not need to seek perfect normality with respect to these distributions, since $t$-based statistical inference procedures are robust to this assumption (as was demonstrated in Question 5 in Problem Set 3).

```{r, eval=T}
epsilon <- 1

par(mfrow=c(2,2))
hist(full.cases$cases, main="2020 cases, untransformed", xlab="2020 cases")
hist(log(full.cases$cases + epsilon), main="2020 cases, log-transformed", 
     xlab="2020 cases")
hist(full.cases$total.cases, main="total.cases, untransformed", 
     xlab="total.cases")
hist(log(full.cases$total.cases + epsilon), main="total.cases, log-transformed",
     xlab="total.cases")
```

The Normal Q-Q Plots below corroborate the conclusion that this assumption is reasonable. In each plot, the data for the most part follows the line that we would expect if it were perfectly normally distributed; although, since the data sits below the line at both the positive and negative extremes of the graph, the left tail is a bit fatter and the right tail a bit skinnier than the perfectly normal distribution in each of these sample distributions. 

```{r, eval=T, fig.height=3}
par(mfrow=c(1,2), cex=0.5)
qqnorm(log(full.cases$cases + epsilon), main="Normal Q-Q Plot, 2020 cases")
qqline(log(full.cases$cases + epsilon))
qqnorm(log(full.cases$total.cases + epsilon), 
       main="Normal Q-Q Plot, total.cases")
qqline(log(full.cases$total.cases + epsilon))
```

## Student-$t$ Tests 

Now that we have shown the assumptions of Student-$t$ tests to be reasonable, we can proceed to perform the tests themselves. We perform two two-sided $t$-tests for a difference in means. In each case, the null hypothesis is that the true mean case counts in the two groups — religiously affiliated and non-religiously affiliated institutions — are equal over the given time period, and the alternative is that they are not equal. As in the rest of the paper, we will use the standard confidence level of $\alpha = 0.05$, for the sake of convenience and conformity with the wider statistics community. Below are the results from these tests. 

```{r, eval=T, render=lemon_print}
cases.t.test <- t.test(log(cases + epsilon) ~ religious, data=full.cases)
total.t.test <- t.test(log(total.cases + epsilon) ~ religious, data=full.cases)

data.frame(sample=c("2020 only", "entire year"), 
           test.statistic=c(cases.t.test$statistic, total.t.test$statistic),
           df=c(cases.t.test[[2]], total.t.test[[2]]),
           p.value=c(cases.t.test$p.value, total.t.test$p.value))
```

In both tests, we see that our p-value is greater than the significance level of 0.05; we thus fail to reject the null hypothesis in both cases — we do not have statistically significant evidence to suggest that the true mean case counts, both for the fall semester and for the entire year, are different between the religiously affiliated and non-religiously affiliated institutions. Note that the degrees of freedom — which are not integers owing to R's use of the Welch approximation — are different for the two tests. This is because the test using the `total.cases` data has fewer observations, since some schools in our sample did not report case counts for the spring semester. This issue is tackled in greater detail below (see "Determining whether data from 2020 or 2020 and 2021 should be used"). 

## Non-Parametric Testing — Wilcox Rank Sum Test

We will also perform the non-parametric Wilcox Rank Sum Test to determine whether the case counts are different between religiously affiliated and non-religiously affiliated schools. The strength of this non-parametric test lies in the fact that it does not rely on an assumption on the distribution of the data-generating process of the sample. Thus, though we are confident in having shown above that the distributions of the log-transformed responses are sufficiently approximately normal, for the sake of completeness, we will perform this non-parametric test, and see if it leads us to the same conclusions. 

Again, we will perform the Wilcox Rank Sum Test on data from just the fall semester and on data from the entire academic year. For the following two tests, the null hypothesis is that the true average quantiles within the two groups — when the data from both groups are ranked together — are the same, while the alternative hypothesis is that there is an association between group status and the average quantile of the observations in the entire population. 

```{r, eval=T, render=lemon_print}
cases.wilcox <- wilcox.test(x = full.cases$cases[full.cases$religious == "Yes"], 
            y = full.cases$cases[full.cases$religious == "No"], 
            alternative='two.sided', exact = FALSE, correct = FALSE, 
            conf.int = TRUE)
total.wilcox<-wilcox.test(x=full.cases$total.cases[full.cases$religious=="Yes"], 
            y = full.cases$total.cases[full.cases$religious == "No"], 
            alternative='two.sided', exact = FALSE, correct = FALSE, 
            conf.int = TRUE)

data.frame(sample=c("2020 only", "entire year"), 
           test.statistic=c(cases.wilcox$statistic, total.wilcox$statistic),
           p.value=c(cases.wilcox$p.value, total.wilcox$p.value))
```

As the results above show, neither test was statistically significant at the 0.05 confidence level; we thus again fail to reject the null hypothesis, concluding that there is no statistically significant evidence to suggest that the there is an association between religious affiliation and average quantile of case counts in either sample. 

# Basic Linear Regression Models

Of course, group tests are limited in that they do not take into account potential confounding variables that might reveal the significance of an institution's having a religious affiliation. To take into account the effects of these potential confounding variables — which we have already identified at length, as evidenced by the extensive list of predictors we compiled before beginning our analyses — we will fit linear models. We will then use these linear models to perform statistical inference on the coefficient of the `religious` predictor, determining whether there is a statistically significant relationship between an institution's religious affiliation and the number of cases that it recorded.

## Checking the Assumptions of Linear Regression Models

Before we can fit more complicated models, we must first check the assumptions of linear regression. We begin by checking the assumption of linearity. To do so, we plotted `cases` versus all of the quantitative predictors that we would like to include in our analyses. We the identified that `total.headcount`, `percent.american.native`, `percent.asian`, `percent.black`, `percent.hispanic.latino`, and `percent.pacific.islander` would benefit from being log-transformed; given the left-skewness of its distribution, we also determined that `percent.fin.aid` would be best transformed using the following transformation `log(100 - percent.fin.aid + 1)`. The following plots show that the distribution of these predictors before and after being transformed. *In these plots, the y-axis is always log-transformed cases* (2020); the axis label is not printed in order to save space. 

```{r, eval=T, fig.height=1.5, message=FALSE}
library(patchwork)
suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=total.headcount)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(total.headcount))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.american.native)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(percent.american.native))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.asian)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(percent.asian))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.black)) + geom_point() + ylab("")) +
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(percent.black))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.hispanic.latino)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(percent.hispanic.latino))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.pacific.islander)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(percent.pacific.islander))) + geom_point() + ylab(""))))

suppressWarnings(print((ggplot(full.cases, aes(y=log(cases + epsilon), x=percent.fin.aid)) + geom_point() + ylab("")) + 
  (ggplot(full.cases, aes(y=log(cases + epsilon), x=log(100-percent.fin.aid+1))) + geom_point() + ylab(""))))
```

We claim that with these transformations, the assumption of linearity is reasonable. 

What remains to be shown is that the assumption of homoskedasticity is reasonable. In order to check this assumption, we fit basic regression models for `cases` and `total.cases` using all of the predictors in our predictor set.

```{r, eval=T, fig.height=3}
lm6 <- lm(log(cases + epsilon) ~ religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree, data=full.cases)

plot(lm6$residuals ~ lm6$fitted.values, main="cases (2020)", xlab="fitted values",
     ylab="residuals", cex=0.5)
abline(h=0, col="gray")
```

As is shown in the plot above, the spread of the residuals is not constant across the entire range of fitted values — thus, it is called into question whether the assumption of homoskedasticity is reasonable in this case. The behavior of this graph is interesting: Towards the left extreme of the graph, there appear to be bands of observations whose positions on this plot form negatively sloped lines. The left-most line corresponds to all those observations where the case counts are zero; the second-left-most line corresponds to the observations where the case counts are one; the next corresponds to the observations where the case count is two, etc. As one moves closer to the center of the plot, this pattern disappears, and the plot assumes a form that is much better suited to linear regression. Thus, this problem arises from the fact that our response variable is discrete, and from the fact that we have so many observations with very low case counts. As it stands, we must be wary of this assumption of homoskedasticity, which might affect the standard errors that we will use to calculate the statistical significance of coefficient estimates. Later in the paper, we will perform a sensitivity analysis using Poisson regression to corroborate the results we obtain using Gaussian regression. 

Below, we recreate the same plot, but for the case counts for the entire 2020-21 academic year. 

```{r, eval=T, fig.height=3}
lm7 <- lm(log(total.cases + epsilon) ~ religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree, data=full.cases)

plot(lm7$residuals ~ lm7$fitted.values, main="total.cases (2020-21)", 
     xlab="fitted values", ylab="residuals", cex=0.5)
abline(h=0, col="gray")
```

In the above plot for the model for `total.cases`, the spread of the residuals appears much more consistent across the entire range of fitted values. If one looks carefully enough, however, they will see that the same behavior observed in the preceding plot is present in this one, the difference being that there are fewer schools that reported very low case counts (0, 1, or 2). This is likely due to the fact that, with an extra five months of data, our response took on enough different values to begin to behave more like a continuous variable; in addition, a not-insignificant number of schools that simply did not report case data for the spring semester (297 observations).

## Determining whether data from 2020 or 2020 and 2021 should be used 

Naturally, there is a temptation to focus on case data for the entire academic year (the entire time period during which case data was collected by the *New York Times*). There is one problem, however: While the *New York Times* was able to compile case data for all of the schools in our data set for the fall semester of that year, it was not able to find data for 297 schools for the spring semester. This is a nontrivial number of observations given that our data set only consists of 1,705 different institutions; thus, to determine whether it would be sound to remove these 297 observations and fit models and perform statistical inferences on data from the entire academic year, we must determine whether the two groups of schools in question — those that reported data for the entire academic year, and those that did not — are sufficiently similar to one another. 

The first step is to compare the coefficients of our baseline model (`lm6` above) when it is fit to all 1705 observations, as well as individually to the two different groups of schools in question. The coefficient estimates, along with the p-values (not adjusted to be heteroskedastically consistent) are given below.

```{r, eval=T}
lm6.both.only <- lm(formula(lm6), 
                    data=full.cases[!is.na(full.cases$cases_2021),])
lm6.twenty.only <- lm(formula(lm6),
                      data=full.cases[is.na(full.cases$cases_2021),])
modelsummary(list("all schools"=lm6, 
                  "schools with data\nfor both years"=lm6.both.only, 
                  "schools with only\n2020 data"=lm6.twenty.only),
             estimate="{estimate}", 
             statistic="{p.value}",
             shape=term ~ model + statistic, 
             gof_omit="IC")
```

As we can see, when the sample of the institutions used to fit the model changes, some of the coefficient estimates change vastly. Take, for example, the coefficient estimate for `religionYes`, which can be interpreted as the natural log of the change in cases that we would expect if a given school were to have a religious affiliation rather than not have one. This coefficient estimate is not statistically significant when all institutions are considered together, is positive and statistically significant when the institutions that reported data for the entire academic year are considered, and negative and statistically significant when the institutions that only reported data for the fall are considered. In fact, if one were to inspect the table above, they would notice that the majority of predictors included in this baseline model experienced changes in their coefficient estimates and statistical significance as the sample of institutions was changed.  

Clearly, then, there are some underlying differences between the schools that did and the schools that did not report case data for the spring semester of the 2020-21 academic year. Consider the coefficient estimate for `religiousYes`, which is quite different in the two models fit to data from only one of the two subgroups; this is especially interesting for our research purposes. Indeed, searching for the differences between these subgroups in terms of `religious` and other predictors might help us to answer the questions about the association between religious affiliation and reported Covid cases that are motivating this paper. 

First, we investigate the difference in the political leanings of the states in which these schools were located, as given by `gap20repub`. To do so, we perform a $t$-test (the assumptions that allow us to do so have been explored above), with the null hypothesis being that the true means `gap20repub` are equal between the two groups of institutions, and the alternative being that the true means are different. The result of this test is shown below. 

\begin{center}
```{r, eval=T, render=lemon_print}
only.twenty <- full.cases[is.na(full.cases$cases_2021),]
both <- full.cases[!is.na(full.cases$cases_2021),]

gap20.test <- t.test(both$gap20repub, only.twenty$gap20repub)
data.frame(predictor=c("gap20repub"), test.statistic=c(gap20.test$statistic),
           df=c(gap20.test[[2]]),
           p.value=c(gap20.test$p.value))
```
\end{center}

With a p-value that is much smaller than our $\alpha =0.05$ (again, because we do not assume equal variances between the groups, the Welch approximation for degrees of freedom is used), we reject the null hypothesis, concluding that the true mean `gap20repub` is different for the schools that did report case data for 2021 versus those that did not. As it turns out, the 95% $t$-based confidence interval for the mean `gap20repub` for the schools that did report 2021 case data minus the mean `gap20repub` for the schools that did not is (-9.468120, -4.959647) — thus, we conclude that the schools that did report 2021 case data are located in states that are considerably more left-leaning than the schools that did not. 

We repeat this sort of analysis for `total.headcount`, `tuition`, `percent.fin.aid`, `mask.mandated.days`, `on.campus.housing`, and `religious`, predictors that we have chosen based on large differences in the coefficient estimates produced by the models above when fit to only one of the two subgroups of institutions. We use a two-sided $t$-test for the quantitative variables and two-sided z-tests for proportions for the `categorical` predictors; we also perform the same transformations as above to make sure that the predictor in question is approximately normally distributed in each test. The hypotheses are similar to the `gap20repub` test, the null being that the true means/proportions are equal between the two groups, and the alternative being that they are not equal. As per the results shown below, the tests for all six of these predictors showed statistically significant differences between the two groups of universities. 

\begin{center}
```{r, eval=T, render=lemon_print}
size.test <- t.test(log(both$total.headcount), log(only.twenty$total.headcount))
tuition.test <- t.test(both$tuition, only.twenty$tuition)
fin.aid.test <- t.test(log(100-both$percent.fin.aid+1), log(100-only.twenty$percent.fin.aid+1))
mask.test <- t.test(both$mask.mandated.days, only.twenty$mask.mandated.days)
on.campus.test <- prop.test(x=matrix(c(sum(both$on.campus.housing == "Yes"), sum(both$on.campus.housing == "No"), 
                     sum(only.twenty$on.campus.housing == "Yes"), 
                     sum(only.twenty$on.campus.housing == "No")), nrow=2, byrow=T), 
          n=c(length(both$on.campus.housing), length(only.twenty$on.campus.housing)))
religious.test <- prop.test(x=matrix(c(sum(both$religious == "Yes"), sum(both$religious == "No"), 
                     sum(only.twenty$religious == "Yes"), 
                     sum(only.twenty$religious == "No")), nrow=2, byrow=T), 
          n=c(length(both$religious), length(only.twenty$religious)))

data.frame(predictor=c("total.headcount", "tuition", "percent.fin.aid",
                       "mask.mandated.days", "on.campus.housing", 
                       "religious"),
           test.statistic=c(size.test$statistic, tuition.test$statistic,
                            fin.aid.test$statistic, mask.test$statistic,
                            on.campus.test$statistic, religious.test$statistic),
           df=c(size.test[[2]], tuition.test[[2]], fin.aid.test[[2]], 
                mask.test[[2]], on.campus.test[[2]], religious.test[[2]]),
           p.value=c(size.test$p.value, tuition.test$p.value, 
                     fin.aid.test$p.value, 
                     mask.test$p.value, on.campus.test$p.value, 
                     religious.test$p.value))
```
\end{center}

The existence of these statistically significant differences between these two groups of institutions leads us to two distinct conclusions: Firstly, that we should perform all further analyses on data only from the fall 2020 semester, and secondly, that we should consider interaction effects in our model to help better isolate and take into account these demonstrated differences between these two different groups of institutions.  

# Linear Models with Interaction Effects

Above we performed several linear regressions with no interactions and multiple predictors. Our "base model" includes all of the predictors we include in this paper, which are: `religious`, `tuition`, `total.headcount`, `percent.american.native`, `percent.asian`, `percent.black`, `percent.hispanic.latino`, `percent.pacific.islander`, `percent.white`, `percent.two.more.races`, `percent.women`, `grad.rate`, `percent.fin.aid`, `on.campus.housing`, `gap20repub`, `private`, `percent.student.loan`, `mask.mandated.days`, `occupational.degree` and `hs.equivelant.degree`. Above we discuss using 2020 cases, 2021 cases, or the total number of cases, concluding that we will use only 2020 cases for the remaining models. 

We will first look at three linear models with all of the predictors in this paper: our "base model," an interaction model between the `religious` predictor and the others (`religiousInteraction`), and a full interaction model (`fullInteraction`). 

```{r}
lm7 <- lm(log(cases + epsilon) ~ religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree, data=full.cases)
lm7summary = summary(lm7)

summary_pval <- lm7summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()

count = 0
for (i in 2:length(summary_pval)){
  if (summary_pval[i] < 0.05){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, lm7summary$coefficients[i, 1])
    count = count + 1
  }
}

data.frame(coefficients = significant_coeff,
           p.value = significant_pval)
           

```

Above is the base model. We see that there are a lot of statistically significant coefficients on the predictors, holding all else constant. These are noted with a star next to them in the above table. Notably, the coefficient for the `religious` variable does not carry statistical significance, with a p-value of .25190, significantly above $\alpha = 0.05$. This could be for a number of reasons, including that 1) Religious affiliation (either Yes: Religious or No: Not Religious) simply has no correlation with 2020 Covid cases or 2) The `religious` variable shares predictive power (has some degree of collinearity) with other predictor variables. 

We will determine this relationship by fitting the `religiousInteraction` model; a linear model that includes the interactions between the religious variable and the remaining confounding variables.

```{r}
religiousInteraction <- lm(log(cases + epsilon) ~ religious*( tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree) - religious:private,
            data=full.cases)

relIn.summary = summary(religiousInteraction)

summary_pval <- relIn.summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()

for (i in 2:length(summary_pval)){
  if (summary_pval[i] < 0.05){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, relIn.summary$coefficients[i, 1])
    count = count + 1
  }
}

data.frame(coefficients = significant_coeff,
           p.value = significant_pval)
           

#summary(religiousInteraction)
```

The `religiousInteraction` model clarifies the significance of the `religious` predictor: When accounting for the interactions between `religious` and other predictors, as seen in this model, `religious` becomes a statistically significant predictor, with a p-value of 0.000290, significantly below the p-value of $\alpha = 0.05$. This indicates that the previous "base model" had `religious` as an insignificant predictor because it interacted with other variables which were collinear and therefore shared a lot of predictive power. Given the number of confounding variables, this result makes sense. We see in this model that the coefficient on the `religiousYes` variable is 3.665, indicating that when looking at a religiously-affiliated school versus one that is not, holding all other variables constant, the religious affiliation alone implies an increase of $e^{3.665}$ Covid cases, which is a significant increase. 

`religious` had a statistically significant interactive term with 5 other variables, which were: `percent.black`, `percent.american.native`, `grad.rate`, `on.campus.housing`, and `mask.mandated.days`. For example, the term `religiousYes:mask.mandated.days` in the summary results of the interaction model had a coefficient of 2.275e-03 and a p-value of 6.43e-05. This implies that it is statistically significant in this model, and for religiously affiliated schools, a one-day increase in mask mandates correlates with an increase in log Covid cases compared to its non-religious counterpart (specifically $e^{0.002275}$. This result is interesting, and could prompt further investigation in a different paper. 

Given the significance of the interaction terms in our partial interaction model, we will now look at a full interaction model with all predictive variables in this paper to predict the 2020 Covid cases. Significant results relating to the `religious` predictor are printed below. 

```{r}
fullInteraction <- lm(log(cases + epsilon) ~ (religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree)^2 - religious:private,
            data=full.cases)

#summary(fullInteraction)
fullIn.summary = summary(fullInteraction)

summary_pval <- fullIn.summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()

for (i in 2:length(summary_pval)){
  predictor_name = substr(names(summary_pval[i]), 1, 9)
  if (summary_pval[i] < 0.05 && predictor_name == "religious"){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, fullIn.summary$coefficients[i, 1])
    count = count + 1
  }
}

data.frame(coefficients = significant_coeff,
           p.value = significant_pval)


```
The `religious` variable in the above model with all interactive terms is not statistically significant in predicting 2020 Covid cases at Universities, with a p-value of 0.290212. But, very few terms alone are significant in the model; only `percent.black`, `percent.two.more.races`, and `on.campus.housing`. Even the predictor `total.headcount` is not statistically significant in this model, whereas it is consistently very significant in other models given the obvious relationship between the total number of people on a given campus and the number of Covid cases amongst them. This result suggests that a lot of the variables are correlated with one another to varying degrees. Thus, there are many interactions, and therefore predictive terms in this model. These have taken the predictive power from the `religious` variable by itself; though some interactions with the `religious` term are statistically significant, such as `percent.asian`, `percent.black`, `mask.mandated.days`, and `on.campus.housing`. This indicates that in relation to 2020 Covid cases, the religious affiliation has an affect on the predictive power of those variables. Ultimately a high number of predictive terms in a model dealing with highly collinear data will inherently diminish the predictive power from a term that shares a lot of correlation with others (referencing `religious`, amongst other terms). With so many predictors, the $t$-tests used to determine the statistical significance of each coefficient estimate are also not very powerful, as the number of degrees of freedom is quite high. 

```{r}
# MAKE THIS INTO A TABLE ?
# I like that idea — Daniel 


data.frame(Model=c("base model", "religiousInteraction", 
                   "fullInteraction"), 
           R.Squared = c(summary(lm7)$r.squared, summary(religiousInteraction)$r.squared
, summary(fullInteraction)$r.squared
))

```

We see in the above table that the $R^2$ value for the three models increases as the number of interactive terms increases. This indicates the significance of the interactive terms in the prediction model. It is unsurprising that the greatest increase is between the `religiousInteraction` model and the `fullInteraction` model. This can be explained by the sheer number of interactive terms in the model, as well as the collinearity between a lot of variables in the data. 

We now perform ESS $F$-tests to compare the predictive power the base model to `religiousInteraction` and `religiousInteraction` to `fullInteraction`. In each test, the null hypothesis is that the true coefficients of the additional predictors included in the larger model — `religiousInteraction` in the first test, and `fullInteraction` in the second — are all zero; the alternative hypothesis is that the true coefficient of one or more of these predictors is not zero, i.e., that the additional predictors provide explanatory power. The result of these ESS $F$-tests are shown below. 

```{r}
# ESS F-test 1
f.test.1 <- anova(lm7, religiousInteraction)

# ESS F-test 2
f.test.2 <- anova(religiousInteraction, fullInteraction)

data.frame(ESS.F.test=c("base model vs. religiousInteraction", 
                  "religiousInteraction vs. fullInteraction"), 
           F.statistic=c(f.test.1$'F'[2], f.test.2$'F'[2]),
           df=c(f.test.1$Df[2], f.test.2$Df[2]),
           p.value=c(f.test.1$`Pr(>F)`[2], f.test.2$`Pr(>F)`[2]))

# UNSURE OF WHAT THESE RESULTS MEAN given that most are very small numbers. 
#vcov7 = vcov(lm7)
#vcov7[2,]
```

In the first test, because the p-values of each test are below our $\alpha=0.05$, we reject the null hypothesis, and conclude that there is evidence that the `religious` interaction terms contribute to the model. Furthermore, in the second ESS F-test between the `religiousInteraction` model and the `fullInteraction` model, we again reject the null hypothesis, concluding that there is evidence that the additional interaction terms in `fullInteraction` have some explanatory power. This indicates that both `religious` interaction with the confounding variables (as discussed above) and the predictors interacting with one another are significant and improve (from the $R^2$ results) their respective models. 

We used a variance-covariance matrix result to view the covariance values between `religious` and all other variables to create a simplified linear regression model with the most correlated confounding variables removed. The refined full interaction model is below, with the terms `private`, `percent.pacific.islander`, `percent.hispanic.latino`, `on.campus.housing`, `percent.native.american`, `gap20repub`, `percent.women`, `grad.rate`, `percent.white`, `percent.student.load`, and `mask.mandated.days` removed. 

```{r}

simple.fullInteraction <- lm(log(cases + epsilon) ~ (religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + 
             percent.two.more.races + 
              log(100-percent.fin.aid+epsilon) + 
            occupational.degree + hs.equivalent.degree)^2, data=full.cases)

sfullIn.summary = summary(simple.fullInteraction)

summary_pval <- sfullIn.summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()

for (i in 2:length(summary_pval)){
  predictor_name = substr(names(summary_pval[i]), 1, 9)
  if (summary_pval[i] < 0.05 && predictor_name == "religious"){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, sfullIn.summary$coefficients[i, 1])
    count = count + 1
  }
}

data.frame(coefficients = significant_coeff,
           p.value = significant_pval)
```

We see above that we have artificially made the `religious` predictor significant in the full interactive model by removing terms that it is collinear with. But, this comes at the expense of the $R^2$ value, now `r sfullIn.summary$r.squared` which has decreased even from the "base model" with all predictors but no interactions. This indicates that the variables we removed are important in the prediction of 2020 Covid cases. 


```{r}

# Not sure if to include; RMSEs are pretty similar and I am getting errors on the interaction models. 
# The errors are coming from overfitting, I think —Daniel 

RMSE <- function(y, yhat) {
     SSE = sum(na.omit(y-yhat)^2)
     return(sqrt(SSE/length(y)))
}

# getting errors so maybe don't use?
RMSE(full.cases$cases, predict(lm7, full.cases))
RMSE(full.cases$cases, predict(religiousInteraction, full.cases))
RMSE(full.cases$cases, predict(simple.fullInteraction, full.cases))

```

# In Search of a Parsimonious Model: Sequential Variable Selection Models

Of course, both `religiousInteraction` and `fullInteraction` are large, complicated models with many predictors — this makes them hard to interpret, and also leaves our significance tests diminished in statistical power. In order to find more parsimonious models — i.e., remove some predictors from these models — we performed Sequential Variable Selection using AIC as our model comparison criterion, stepping backward, forward, and in both directions. Brief descriptions and results from each resulting model are below. 

*Backward step model:* For this model, we started with `fullInteraction`, stepping backwards until a reduction in AIC was no longer possible. This process removed 116 predictors, leaving the resulting model, `back.step`, with 94 predictors. This list includes `religiousYes`, as well as interactions between `religious` and eight other predictors: `tuition`, `log(total.headcount + epsilon)`, `log(percent.american.native + epsilon)`, `log(percent.asian + epsilon)`, `log(percent.black + epsilon)`, `percent.two.more.races`, `on.campus.housingYes`, and `mask.mandated.days`. Of the nine predictors related to `religious` included in this model, only 5 were significant at the 0.05 level: `religiousYes:tuition`, `religiousYes:percent.two.more.races	`, `religiousYes:log(percent.asian + epsilon)`, `religiousYes:on.campus.housingYes`, and `religiousYes:mask.mandated.days`, as shown in the table below. Thus, according to this model, the statistically significant relationship between `religious` and `cases` changes in nature based on the predictors `log(percent.asian + epsilon)`, `on.campus.housingYes`, `mask.mandated.days`, `percent.two.more.races`, and `tuition`. 

\begin{center}
```{r, eval=T, cache=T, render=lemon_print}
back.step <- step(fullInteraction, direction = "backward", k=2, trace=0)

back.summary = summary(back.step)

summary_pval <- back.summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()

for (i in 2:length(summary_pval)){
  predictor_name = substr(names(summary_pval[i]), 1, 9)
  if (summary_pval[i] < 0.05 && predictor_name == "religious"){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, back.summary$coefficients[i, 1])
    count = count + 1
  }
}

back.df = data.frame(coefficients = significant_coeff,
           p.value = significant_pval)

back.df
```
\end{center}

*Forward step*: For this model, we started with our base model, and stepped forward until a reduction in AIC was no longer possible. This process added 55 predictors, leaving `forward.step` with 75 predictors. In terms of the predictors related to `religious`, this list included `religiousYes`,  `religiousYes:mask.mandated.days`, `religiousYes:on.campus.housingYes`, `religiousYes:tuition`, `religiousYes:log(percent.asian + epsilon)`, and `religiousYes:percent.two.more.races`. Interestingly, all of the `religious` interaction terms in this model were statistically significant at the 0.05 level, but the predictor `religiousYes` itself was not. We can interpret this model as saying that whether a school has a religious affiliation changes the relationship between `cases` and these other predictors, but that `religious` does not have a significant association with `cases` in of itself.  

\begin{center}
```{r, eval=T, cache=T, render=lemon_print}
forward.step <- step(lm7, scope = list(upper = formula(fullInteraction)), 
                     direction = "forward", k=2, trace=0)
forward.summary = summary(forward.step)

summary_pval <- forward.summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()

for (i in 2:length(summary_pval)){
  predictor_name = substr(names(summary_pval[i]), 1, 9)
  if (summary_pval[i] < 0.05 && predictor_name == "religious"){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, forward.summary$coefficients[i, 1])
    count = count + 1
  }
}

forward.df = data.frame(coefficients = significant_coeff,
           p.value = significant_pval)

forward.df
```
\end{center}

Forward step (from back): For this model, we started with the back step model (described above, with 94 predictors), and stepped forward until a reduction in AIC was no longer possible. This process did not actually add any predictors to the back step model, leaving `forward.step` with 94 predictors. But, the significance of each predictor changed, as only three `religious` interaction terms are statistically significant in this model. These terms are `religiousYes:mask.mandated.days`, `religiousYes:on.campus.housingYes` and `religiousYes:log(percent.asian + epsilon)`.

```{r, eval=T, cache=T}
forward.step2 <- step(back.step, scope = list(upper = formula(fullInteraction)), 
                      direction = "forward", k=2, trace=0)

forward2.summary = summary(forward.step2)

summary_pval <- forward2.summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()

for (i in 2:length(summary_pval)){
  predictor_name = substr(names(summary_pval[i]), 1, 9)
  if (summary_pval[i] < 0.05 && predictor_name == "religious"){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, forward2.summary$coefficients[i, 1])
    count = count + 1
  }
}

forward2.df = data.frame(coefficients = significant_coeff,
           p.value = significant_pval)

forward2.df

```

Forward step (from intercept): For this model, we stepped forward from an intercept model. As a result, `forward.step1` incorporated 67 predictors. This list of predictors did not include `religiousYes`, or any interaction term with `religious`. This could be taken as evidence that `religious` really is not a relevant predictor at all, at least compared to the many other available predictors and their interactions.

```{r, eval=T, cache=T}
df <- full.cases[,c("cases", "religious", "tuition", "total.headcount", "percent.american.native", "percent.asian", "percent.black", "percent.hispanic.latino", "percent.pacific.islander", "percent.white", "percent.two.more.races", "percent.women", "grad.rate", "percent.fin.aid", "on.campus.housing", "gap20repub", "private", "percent.student.loan", "mask.mandated.days", "occupational.degree", "hs.equivalent.degree")]
df <- df[complete.cases(df),]

model0 <- lm(log(cases+epsilon) ~ 1, df)

forward.step1 <- step(model0, scope = list(upper = formula(fullInteraction)), 
                      direction = "forward", k=2, trace=0)
```

Bidirectional step: For this model, we used our base model as our starting point, and stepped in both directions (alternating forwards and backwards), until an improvement in AIC was no longer possible. As a result, 55 predictors were added to the base model to produce `both.step`, a model with 75 total predictors. This list of predictors for this model was extremely similar to the list of predictors included in the `forward.step` model (from the "base model"); only four predictors were different between the two models, none of which were related to `religious`. Predictably, the same interaction terms with `religious` that were significant in `forward.step` were statistically significant in this model as well (noted below), and `religiousYes` was again not statistically significant. This model corroborates the idea that whether an institution is religious changes the relationship between `cases` and these other predictors, but that there is no relationship between the predictor `religious` and the response `cases`. 

```{r, eval=T, cache=T}
both.step = step(lm7, scope = list(lower = formula(model0), upper = formula(fullInteraction)),
            direction = "both", k=2, trace=0)

both.summary = summary(both.step)

summary_pval <- both.summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()

for (i in 2:length(summary_pval)){
  predictor_name = substr(names(summary_pval[i]), 1, 9)
  if (summary_pval[i] < 0.05 && predictor_name == "religious"){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, both.summary$coefficients[i, 1])
    count = count + 1
  }
}

both.df = data.frame(coefficients = significant_coeff,
           p.value = significant_pval)
both.df
```

The results of the five step models explored come to very similar conclusions: though `religious` alone was sometimes included as a predictor and sometimes not, it was never statistically significant in the models. Conversely, five interactive terms stood out, namely, `religiousYes:mask.mandated.days`, `religiousYes:on.campus.housingYes`, `religiousYes:tuition`, `religiousYes:log(percent.asian + epsilon)`, and `religiousYes:percent.two.more.races`. Three of these (`mask.mandated.days`, `on.campus.housing`, and `percent.asian`) were statistically significant in four of the models, and the remaining two were significant in three of those four. The fifth model, the forward interaction model starting from the intercept model, had no statistically significant interactive terms with `religious`. Of these interactions, `mask.mandated.days` was always the most statistically significant with `religious`. Next was `on.campus.housing`, then typically `percent.asian`. The significance of these interaction terms can be interpreted as an indication that the relationship between case counts and the religious affiliation of a school changes based on the values of these other predictors. 

```{r}
aic.results = data.frame(c("Backward Step", "Forward (from base model)", "Forward (from backward model)", "Forward (from intercept model)", "Both Directions"), c(AIC(back.step), AIC(forward.step), AIC(forward.step2), AIC(forward.step1), AIC(both.step)))
colnames(aic.results) = c("Model", "AIC")
aic.results
```

We see in the above table that the models with the lowest AIC (this indicates a "better" model) is the Backward Step model and the Forward (from the backward model). These are the same models, as discussed above, as the forward procedure did not add any variables to the starting backward model. Notably, these included all five discussed interactive terms with `religious`. Conversely, the model with the greatest AIC was the Forward direction model starting from the intercept model. This was the model without the `religious` interaction terms. 

We thought that the backward step model may be the worst for our data due to the high collinearity, as the procedure would force the model to consider and keep collinear variables because it starts with them. Conversely, a forward stepwise model could entirely omit them from the procedure. While the run time for the backward step model was the longest and is indicative of considering so many interactive terms, it has a higher AIC and does not seem to differ much with variable significance from other step models we considered. 

# LASSO for Variable Section

Because sequential variable selection can be sensitive to outliers and high-leverage observations, and in order to get a better idea of which of the three sequential variable selection models might be the most reliable, we will employ an alternative to this method: LASSO. A penalized form of regression, LASSO is able to function as a method of variable selection, since, unlike ridge regression, it actually does shrink coefficient estimates to exactly zero. 

We will apply LASSO to both the `religiousInteraction` and `fullInteraction` models, and interpret the results of each model. In each case, we will use cross-validation to select the best regularizing constant $\lambda$ with which to fit each model. 

First, we do so for the `religiousInteraction` model. We plot the $\lambda$ values that we cross validated to make sure that the minimum value given by R is not the lower or upper bound of those considered (which would suggest that we widen our range of $\lambda$ values considered); we then fit a LASSO model using the formula from `religiousInteraction`; call this model LASSO Model 1. 

```{r, eval=T, fig.height=4}
lambda.range <- 10^seq(-10, 3, 0.1)
rI.cv <- cv.glmnet(model.matrix(religiousInteraction), df$cases, 
                   lambda=lambda.range, alpha=1, trace=0)
plot(rI.cv)
rI.lasso <- glmnet(model.matrix(religiousInteraction), df$cases,
                   lambda=rI.cv$lambda.min, alpha=1, trace=0)
rI.lasso.coef <- data.frame(coefficient=colnames(model.matrix(religiousInteraction)), beta=matrix(rI.lasso$beta)[,1])
# rI.lasso.coef$coefficient[rI.lasso.coef$beta == 0]

rI.lasso.2 <- glmnet(model.matrix(religiousInteraction), df$cases,
                   lambda=lambda.range, alpha=1, trace=0)
```

In LASSO Model 1, only the intercept estimate and the coefficient estimate for `religiousYes:percent.women` were shrunk all the way to zero. We now perform the same procedure with the `fullInteraction` model, creating LASSO Model 2: 

```{r, eval=T, fig.height=4}
fI.cv <- cv.glmnet(model.matrix(fullInteraction), df$cases, 
                   lambda=lambda.range, alpha=1, trace=0)
plot(fI.cv)
fI.lasso <- glmnet(model.matrix(fullInteraction), df$cases,
                   lambda=fI.cv$lambda.min, alpha=1, trace=0)
fI.lasso.coef <- data.frame(coefficient=colnames(model.matrix(fullInteraction)), beta=matrix(fI.lasso$beta)[,1])
# fI.lasso.coef$coefficient[rI.lasso.coef$beta == 0]
```

This time around, there are 11 coefficient estimates that were shrunk all the way to zero; of those related to `religious`, only `religiousYes:percent.women` was shrunk to zero. 

In order to determine which of the remaining predictors in each model are statistically significant, we refit a linear model using all the coefficients that were not shrunk to zero in each of LASSO Models 1 and 2. Note that even though the intercept was shrunk to zero by LASSO in both cases, we still include an intercept in this new model to make interpretation of the coefficient estimates in the resulting models easier. Call the new linear models LASSO-Selected Linear Model 1 and LASSO-Selected Linear Model 2. Tables 3 and 4 show the coefficients that we found to be statistically significant in each of these two models. 

\begin{center}
```{r, eval=T, render=lemon_print, caption="LASSO-Selected Linear Model 1"}
rI.lasso.remaining <- lm(log(cases + epsilon) ~ religious*(tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree) -
              religious:percent.women, data=full.cases)
rI.stat.sig <- names(summary(rI.lasso.remaining)$coefficients[summary(rI.lasso.remaining)$coefficients[,"Pr(>|t|)"] < 0.05,1])

rI.remaining.summary = summary(rI.lasso.remaining)
summary_pval <- rI.remaining.summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()
count = 0
for (i in 2:length(summary_pval)){
  predictor_name = substr(names(summary_pval[i]), 1, 9)
  if (summary_pval[i] < 0.05) {
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, rI.remaining.summary$coefficients[i, 1])
    count = count + 1
  }
}

data.frame(coefficients = significant_coeff,
           p.value = significant_pval)
```

```{r, eval=T, render=lemon_print, caption="LASSO-Selected Linear Model 2"}
fI.lasso.remaining <- lm(log(cases + epsilon) ~ (religious + tuition + 
            log(total.headcount+epsilon) + 
            log(percent.american.native+epsilon) + log(percent.asian+epsilon) + 
            log(percent.black+epsilon) + log(percent.hispanic.latino+epsilon) + 
            log(percent.pacific.islander+epsilon) +
            percent.white + percent.two.more.races + 
            percent.women + grad.rate + log(100-percent.fin.aid+epsilon) + 
            on.campus.housing + gap20repub + 
            private + percent.student.loan + mask.mandated.days +
            occupational.degree + hs.equivalent.degree)^2 - 
              religious:percent.women - tuition:log(total.headcount + epsilon) -
              log(total.headcount + epsilon):private - 
              log(percent.american.native + epsilon):percent.two.more.races -
              log(percent.black + epsilon):percent.women - 
              log(percent.hispanic.latino + epsilon):log(percent.pacific.islander +
                                                           epsilon) -
              percent.white:gap20repub - percent.two.more.races:gap20repub -
              on.campus.housing:gap20repub - gap20repub:hs.equivalent.degree,
            data=full.cases)
fI.stat.sig <- names(summary(fI.lasso.remaining)$coefficients[summary(fI.lasso.remaining)$coefficients[,"Pr(>|t|)"] < 0.05,1])

fI.remaining.summary = summary(fI.lasso.remaining)
summary_pval <- fI.remaining.summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()
count = 0
for (i in 2:length(summary_pval)){
  predictor_name = substr(names(summary_pval[i]), 1, 9)
  if (summary_pval[i] < 0.05) {
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, fI.remaining.summary$coefficients[i, 1])
    count = count + 1
  }
}
data.frame(coefficients = significant_coeff,
           p.value = significant_pval)
```
\end{center}

In both LASSO-Selected Linear Models 1 and 2, we find that `religiousYes` is statistically significant. Recall that this is the categorical indicator variable for `religious`, which can be interpreted as the natural logarithm of the multiplicative change in cases that we would expect for an institution with a given predictor set if it were to have a religious affiliation, rather than not have one, and all the other predictors were to remain constant.

Interestingly, while five different `religious` interaction coefficients are statistically significant in LASSO-Selected Linear Model 1, only one `religious` interaction coefficient is statistically significant in LASSO-Selected Linear Model 2: `religiousYes:mask.mandated.days`. Thus, while the first model suggests that the relationship between `religious` and `cases` is affected by the value of five other predictors in the model, the second suggests that the association between `religious` and `cases` changes based on the value of `mask.mandated.days` of a given observation. It appears that the interaction effects deemed to be statistically significant in LASSO-Selected Linear Model 1 shared some level of collinearity with some of the additional interaction effects that were included in LASSO-Selected Linear Model 2. In this way, when these other interaction effects were included in the model, the `religious` interaction effects were no longer statistically significant, as the relationships that they helped to describe were better captured by other predictors. 

The plot below demonstrates that there does indeed exist collinearity between the predictors of LASSO Model 2 (and thus LASSO-Selected Model 2). We have chosen to present this plot, because it is more readable than a printout of a massive variance-covariance matrix. This plot shows the trajectories of the coefficient estimates of LASSO Model 2 as the regularizing constant $\lambda$ increases on a logarithmic scale. As is clear, not all of the coefficient estimates are shrunk uniformly towards zero; instead, some display sharp increases in magnitude as $\lambda$ increases. This demonstrates that there is collinearity between some of the predictors: As one predictor in a collinear pair is shrunk to zero, the magnitude of the coefficient estimate for the other increases in order to make up for the lost predictive power. 

```{r, eval=T, fig.height=4}
fI.lasso.2 <- glmnet(model.matrix(fullInteraction), df$cases, 
                   lambda=lambda.range, alpha=1, trace=0)
betas <- fI.lasso.2$beta
log_lam <- log(fI.lasso.2$lambda)
plot(betas[1,] ~ log_lam, type="l", ylab=expression(beta), 
     xlab=expression(log(lambda)), ylim=c(-150,150), xlim=c(-7, max(log_lam)),
     main="LASSO Model 2: Coefficient Estimate Trajectories")
for (i in 2:dim(betas)[1]) {
  lines(betas[i,]~log_lam, lty=i, col=i)
}
```

In the end, we lean towards accepting the conclusions presented by the second model: With a wider range of interaction effects to consider, it is less likely that the statistically significant predictors were only marked as such because they help explain a relationship between `cases` and another predictor with which they are collinear. The conclusion of the LASSO models alone appears to be that there is a statistically significant relationship between `religious` and `cases`, and that the nature of this relationship changes based on the value of `mask.mandated.days`. Interestingly, in LASSO-Selected Model 2, the interaction coefficient `religiousYes:mask.mandated.days` has a positive coefficient: An increase in the number of mask mandated days during the 2020-21 academic year is associated, in this model, with an increase in the multiplicative change that we would expect for a given school if that school were to have a religious affiliation, compared to if that same school were to not have a religious affiliation. This phenomenon also seems to be present in LASSO-Selected Model 1, in which the coefficient estimate for `religious:mask.mandated.days` is also positive. 

# Hierarchical Multi-level Models

We next chose to explore the relationship between 2020 Covid cases and our list of predictors using hierarchical multi-level models, namely the mixed effects models from the `lme4` package in R. We thought this model type would work well with our data because the data can be organized hierarchically as follows: The first level is at the university level; each university has predictors, such as `grad.rate` and `religious`, which are specific to each university. The second level in the hierarchy is the state level, where Universities in each specific state will share cultural, legislative, etc. characteristics that can be vaguely accounted for by sorting the "groups" by states. For example, mask mandates, which were decided by state, were in effect at that stage of the pandemic. Finally, these models have another advantageous capability of being able to account for "weight," meaning we can account for the affect `total.headcount` has on the number of Covid cases. 

A capability of the `lme4` mixed effects package is that it can produce both random intercept and random slope models. We decided that with the data we have, we can only look at random intercept mixed effects models. This is due to the number of observations we have; there are a total of 1855 Universities, which means that in each group (by state + Washington DC), there is an average of just over 36 data points per group. Especially given the differing densities of Universities (there are so many in Massachusetts), there are simply not enough observations per group to justify using and analyzing a random slope model. Fortunately, the nature of these mixed effects models means they will not overfit to the fewer observations in a given group.  

Massive model - this is what I am unsure on, if this is a valid use of all of the predictors.

First we fit a model with all of the predictors from our "base model", except `total.headcount`, which we accounted for using the "weights = " functionality of the `lmer` modelling. We used a random intercept model and grouped by state, meaning that the results include the coefficients for each predictor, which are fixed by state, as well as the intercept, which should differ by state. Below is a model of the coefficient values, which we will use to best visualize a line graph. 

```{r}
mer1 = lmer(log(cases + 1) ~ 1 + religious + tuition + 
    log(percent.american.native + epsilon) + log(percent.asian + 
    epsilon) + log(percent.black + epsilon) + log(percent.hispanic.latino + 
    epsilon) + log(percent.pacific.islander + epsilon) + percent.white + 
    percent.two.more.races + percent.women + grad.rate + log(100 - 
    percent.fin.aid + epsilon) + on.campus.housing + gap20repub + 
    private + percent.student.loan + mask.mandated.days + occupational.degree + 
    hs.equivalent.degree + (1|state), data=full.cases,
    weights=total.headcount/1000, REML=FALSE)

plot_model(mer1)
```
We see in the above model that certain coefficent estimates are significantly larger than the others. It is significant to note that `religiousYes` actualy has a negative affect on the 2020 Covid cases in this model, which is contrary to all previous models we have explored. 

We chose `percent.black` as for the x-axis below to visualize the process for this random intercept model because unlike `private` and `on.campus.housing`, it is a continuous predictor, and unlike `percent.pacific.islander` and `percent.american.native`, it does not have a lot of Universities with 0%. Otherwise, it has the largest coefficient, and will therefore create a more dramatic graph.  

```{r}
color_list = seq(0,1,0.019)

full.cases.col = full.cases
full.cases.col$col = with(
  full.cases, ifelse(religious == "Yes", rgb(1,0,0,.3), rgb(0,0,1,.3)))
  

plot(log(full.cases$percent.black+1), log(full.cases$cases+1), col = full.cases.col$col, ylim = c(0, 10), xlim = c(0,6))
for (i in 1:length(ranef(mer1)[[1]][,1])) {
  intercept = ranef(mer1)[[1]][i,1]
  abline(fixef(mer1)[1]+intercept, fixef(mer1)[6], col = rgb(0,color_list[i], 1-color_list[i]/2, .6))
}

```
The above plot explores the relationship between `percent.black` and 2020 Covid cases in institutions, as well as plots the lines for each state (and DC) generated from our mixed effects (random intercept) model. We can see that the states have some spread in terms of differing intercepts. We can also see that, similar to the interaction models we explored, there is no significant visual relationship between religious institutions (red dots) and non-religious (blue dots) when comparing `percent.black` and `full.cases`. 

Next we explore the relationship between `gap20repub` and `religious` and their interactive effect on 2020 Covid cases grouped by state (still using mixed effects, random intercept). We are exploring this given the feature of the mixed effects models in exploring predictors at the second level of the heirarchical model (ie. the state-level). `gap20repub` is unique by state, but not by institutions in each state. We formally test below whether the effect of university religious affiliation (on 2020 Covid cases) depends on `gap20repub`:

```{r}
lmer3 = lmer(log(cases+1)~gap20repub*religious+(1|state),data=full.cases,weights=total.headcount/1000, REML=FALSE)

lmer3b = lmer(log(cases+1)~gap20repub+religious+(1|state),data=full.cases,weights=total.headcount/1000, REML=FALSE)
anova(lmer3b,lmer3)

red_or_blue = function(gap){
  if (gap < 0) {
    return(rgb(0,0,1,.5))
  }
  else{return(rgb(1,0,0,.5))}
}

plot(full.cases$gap20repub, log(full.cases$cases+1), col = rgb(.5,.5,.5,.2))
for (i in 1:length(ranef(lmer3b)[[1]][,1])) {
  intercept = ranef(lmer3b)[[1]][i,1]
  abline(fixef(lmer3b)[1]+intercept, fixef(lmer3b)[2], col = red_or_blue(voter.gap$gap20repub[i]))
}

```

The result of the anova test between nested models mixed effects models (one with interactive effect, one without) indicates that there is evidence that the interaction between `gap20repub` and `religious` is significant in the predictive model, and the effect of religious affiliation upon 2020 Covid cases does depend on `gap2020repub`. Though this was not flagged as a particularly significant interaction in the above models, it is not surprising, per se: It is clear that the `religious` significantly interacts with many other predictors. 

# Sensitivity Analysis using Poisson Regression

In "Checking the Assumptions of Linear Regression Models", we found behavior in our data set that calls into question the assumption of homoskedasticity that is used in all of the above methods, which are based on Gaussian regression. Because our response is count data, Poisson regression is an obvious candidate for improving the violation of this assumption. The first thing we do to verify this fact is to fit a Poisson regression model using the same formula as our base model from above; as the residuals versus fitted values plot below shows, Poisson regression does, for the most part, fix the intractable violations of homoskedasticity that we encountered with Gaussian regression. 

```{r, eval=T, fig.height=4}
p1 <- glm(cases ~ religious + tuition + log(total.headcount + 
    epsilon) + log(percent.american.native + epsilon) + log(percent.asian + 
    epsilon) + log(percent.black + epsilon) + log(percent.hispanic.latino + 
    epsilon) + log(percent.pacific.islander + epsilon) + percent.white + 
    percent.two.more.races + percent.women + grad.rate + log(100 - 
    percent.fin.aid + epsilon) + on.campus.housing + gap20repub + 
    private + percent.student.loan + mask.mandated.days + occupational.degree + 
    hs.equivalent.degree, data=full.cases, family="poisson")
plot(p1, which=1)
```

With this model, however, the problem is not completely solved: Because the spread in the responses seems to increase as fitted values increase, it seems that weighted regression would be the best approach in the Poisson setting. The most obvious candidate for weights is the predictor `total.headcount` — it is reasonable to think that as school size increases, the differences approaches to handling the outbreak of Covid-19 might be manifested in increasingly different case count numbers. The below plot shows that employing weighted-least-squares Poisson regression using `1/total.headcount` as weights for the most part fixes the problem of heteroskedasticity. 

```{r, eval=T, fig.height=4}
p2 <- glm(formula(p1), data=full.cases, family="poisson", 
          weights=1/total.headcount)
plot(p2, which=1)
```

Clearly, there are still some outliers, which might be due to a number of factors, including exceptionally frequent testing, comprehensive Covid contact tracing systems, superspreader events on campus, or simply exceptionally large outbreaks on campus; however, the assumption of homoskedasticity now seems to be reasonable enough to trust our standard errors. 

We now fit Poisson regression models using the same formulas as the most notable Gaussian models, and compare the lists of coefficient estimates related to religious affiliation marked as statistically significant by the two approaches. Note that when determinging the statistical significance of a coefficient estimate in the Poisson models, we use heteroskedastic-consistent standard errors to account for possible violations of distribution assumption that the mean of the response is equal to its variance. The tables below illustrates the results of these analyses. 

```{r, eval=T, render=lemon_print, caption="Base Model: Gaussian vs. Poisson"}
lm6.summary = summary(lm6)

summary_pval <- lm6.summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()

for (i in 2:length(summary_pval)){
  if (summary_pval[i] < 0.05){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, lm6.summary$coefficients[i, 1])
  }
}

p.compare.df <- data.frame("Gaussian Estimate"=significant_coeff, 
                           "Gaussian p.value"=significant_pval)
p.compare.df$coefficient <- rownames(p.compare.df)

cov.p2 <- vcovHC(p2, type="HC0")
std.err <- sqrt(diag(cov.p2))
r.est <- data.frame("Poisson Estimate"=unname(coef(p2)), 
                    Poisson.p.value= 2 * pnorm(abs(coef(p2)/std.err), lower.tail=FALSE))
r.est$coefficient <- rownames(r.est)

merge(p.compare.df, r.est, by="coefficient", all.x=TRUE)
```

In the base model, only one coefficient that was statistically significant in the Gaussian model was no longer statistically significant in the Poisson model: `percent.student.loan`. Otherwise, though the p-values may have changed, the predictors that were statistically significant in the Gaussian model were statistically significant in the Poisson model as well. 

```{r, eval=T, render=lemon_print, caption="Full Interaction  Model: Gaussian vs. Poisson\n(Only predictors related to `religious`)"}
fullInteraction.poisson <- glm(cases ~ (religious + tuition + log(total.headcount + 
    epsilon) + log(percent.american.native + epsilon) + log(percent.asian + 
    epsilon) + log(percent.black + epsilon) + log(percent.hispanic.latino + 
    epsilon) + log(percent.pacific.islander + epsilon) + percent.white + 
    percent.two.more.races + percent.women + grad.rate + log(100 - 
    percent.fin.aid + epsilon) + on.campus.housing + gap20repub + 
    private + percent.student.loan + mask.mandated.days + occupational.degree + 
    hs.equivalent.degree)^2 - religious:private, 
    data=full.cases, family="poisson")

fI.p.summary = summary(fullInteraction)

summary_pval <- fI.p.summary$coefficients[ , 4]
significant_pval = c()
significant_coeff = c()
terms = c()

for (i in 2:length(summary_pval)){
  predictor_name = substr(names(summary_pval[i]), 1, 9)
  if (summary_pval[i] < 0.05 && predictor_name == "religious"){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, fI.p.summary$coefficients[i, 1])
  }
}

p.compare.df <- data.frame("Gaussian Estimate"=significant_coeff, 
                           "Gaussian p.value"=significant_pval)
p.compare.df$coefficient <- rownames(p.compare.df)

cov <- vcovHC(fullInteraction.poisson, type="HC0")
std.err <- sqrt(diag(cov))
r.est <- data.frame("Poisson Estimate"=unname(coef(fullInteraction.poisson)), 
                    Poisson.p.value= 2 * pnorm(abs(coef(fullInteraction.poisson)/std.err), lower.tail=FALSE))
r.est$coefficient <- rownames(r.est)

merge(p.compare.df, r.est, by="coefficient", all.x=TRUE)
```

With respect to the `fullInteraction` model, of the linear or interaction effects related to the predictor `religious`, two of the four coefficient estimates remained statistically significant in the Poisson model — `religiousYes:log(percent.asian + epsilon)` and `religiousYes:on.campus.housingYes` — while the other two did not. 

```{r, eval=T, render=lemon_print, caption="LASSO-Selected Model 2: Gaussian vs. Poisson\n(Only predictors related to `religious`)"}
fI.lr.poisson <- glm(cases ~ (religious + tuition + log(total.headcount + 
    epsilon) + log(percent.american.native + epsilon) + log(percent.asian + 
    epsilon) + log(percent.black + epsilon) + log(percent.hispanic.latino + 
    epsilon) + log(percent.pacific.islander + epsilon) + percent.white + 
    percent.two.more.races + percent.women + grad.rate + log(100 - 
    percent.fin.aid + epsilon) + on.campus.housing + gap20repub + 
    private + percent.student.loan + mask.mandated.days + occupational.degree + 
    hs.equivalent.degree)^2 - religious:percent.women - tuition:log(total.headcount + 
    epsilon) - log(total.headcount + epsilon):private - log(percent.american.native + 
    epsilon):percent.two.more.races - log(percent.black + epsilon):percent.women - 
    log(percent.hispanic.latino + epsilon):log(percent.pacific.islander + 
        epsilon) - percent.white:gap20repub - percent.two.more.races:gap20repub - 
    on.campus.housing:gap20repub - gap20repub:hs.equivalent.degree 
    - religious:private, 
    data=full.cases, family="poisson", weights=1/total.headcount)

fI.lr.p.summary = summary(fI.lasso.remaining)

summary_pval <- fI.lr.p.summary$coefficients[,4]
significant_pval = c()
significant_coeff = c()
terms = c()

for (i in 2:length(summary_pval)){
  predictor_name = substr(names(summary_pval[i]), 1, 9)
  if (summary_pval[i] < 0.05 && predictor_name == "religious"){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, fI.lr.p.summary$coefficients[i, 1])
  }
}

p.compare.df <- data.frame("Gaussian Estimate"=significant_coeff, 
                           "Gaussian p.value"=significant_pval)
p.compare.df$coefficient <- rownames(p.compare.df)

cov <- vcovHC(fI.lr.poisson, type="HC0")
std.err <- sqrt(diag(cov))
r.est <- data.frame("Poisson Estimate"=unname(coef(fI.lr.poisson)), 
                    Poisson.p.value= 2 * pnorm(abs(coef(fI.lr.poisson)/std.err), lower.tail=FALSE))
r.est$coefficient <- rownames(r.est)

merge(p.compare.df, r.est, by="coefficient", all.x=TRUE)
```

In the Poisson regression version of LASSO-Selected Model 2, only the coefficient estimate for `religiousYes` remained statistically significant; `religiousYes:mask.mandated.days` was no longer statistically significant after we assumed that the errors took on the Poisson distribution. 

\small
```{r, eval=T, render=lemon_print, caption="Backwards Sequential Variable Selection Model: Gaussian vs. Poisson\n(Only predictors related to `religious`)"}
back.step.p <- glm(cases ~ religious + tuition + log(total.headcount + 
    epsilon) + log(percent.american.native + epsilon) + log(percent.asian + 
    epsilon) + log(percent.black + epsilon) + log(percent.hispanic.latino + 
    epsilon) + log(percent.pacific.islander + epsilon) + percent.white + 
    percent.two.more.races + percent.women + grad.rate + log(100 - 
    percent.fin.aid + epsilon) + on.campus.housing + gap20repub + 
    private + percent.student.loan + mask.mandated.days + occupational.degree + 
    hs.equivalent.degree + religious:tuition + religious:log(total.headcount + 
    epsilon) + religious:log(percent.american.native + epsilon) + 
    religious:log(percent.asian + epsilon) + religious:log(percent.black + 
    epsilon) + religious:percent.two.more.races + religious:on.campus.housing + 
    religious:mask.mandated.days + tuition:log(percent.american.native + 
    epsilon) + tuition:log(percent.asian + epsilon) + tuition:log(percent.black + 
    epsilon) + tuition:percent.white + tuition:percent.two.more.races + 
    tuition:grad.rate + tuition:gap20repub + tuition:private + 
    tuition:percent.student.loan + tuition:occupational.degree + 
    log(total.headcount + epsilon):log(percent.black + epsilon) + 
    log(total.headcount + epsilon):grad.rate + log(total.headcount + 
    epsilon):private + log(total.headcount + epsilon):mask.mandated.days + 
    log(percent.american.native + epsilon):log(percent.asian + 
        epsilon) + log(percent.american.native + epsilon):log(percent.hispanic.latino + 
    epsilon) + log(percent.american.native + epsilon):percent.two.more.races + 
    log(percent.american.native + epsilon):grad.rate + log(percent.american.native + 
    epsilon):gap20repub + log(percent.american.native + epsilon):private + 
    log(percent.american.native + epsilon):mask.mandated.days + 
    log(percent.asian + epsilon):log(percent.black + epsilon) + 
    log(percent.asian + epsilon):percent.women + log(percent.asian + 
    epsilon):grad.rate + log(percent.asian + epsilon):log(100 - 
    percent.fin.aid + epsilon) + log(percent.asian + epsilon):gap20repub + 
    log(percent.asian + epsilon):mask.mandated.days + log(percent.asian + 
    epsilon):hs.equivalent.degree + log(percent.black + epsilon):percent.two.more.races + 
    log(percent.black + epsilon):percent.women + log(percent.black + 
    epsilon):gap20repub + log(percent.black + epsilon):hs.equivalent.degree + 
    log(percent.hispanic.latino + epsilon):percent.white + log(percent.hispanic.latino + 
    epsilon):grad.rate + log(percent.hispanic.latino + epsilon):hs.equivalent.degree + 
    log(percent.pacific.islander + epsilon):percent.women + log(percent.pacific.islander + 
    epsilon):gap20repub + log(percent.pacific.islander + epsilon):hs.equivalent.degree + 
    percent.white:percent.two.more.races + percent.white:on.campus.housing + 
    percent.white:mask.mandated.days + percent.two.more.races:grad.rate + 
    percent.two.more.races:log(100 - percent.fin.aid + epsilon) + 
    percent.two.more.races:mask.mandated.days + percent.two.more.races:occupational.degree + 
    percent.two.more.races:hs.equivalent.degree + percent.women:log(100 - 
    percent.fin.aid + epsilon) + percent.women:on.campus.housing + 
    percent.women:gap20repub + percent.women:percent.student.loan + 
    grad.rate:log(100 - percent.fin.aid + epsilon) + grad.rate:gap20repub + 
    grad.rate:percent.student.loan + grad.rate:hs.equivalent.degree + 
    log(100 - percent.fin.aid + epsilon):on.campus.housing + 
    log(100 - percent.fin.aid + epsilon):occupational.degree + 
    on.campus.housing:gap20repub + on.campus.housing:percent.student.loan + 
    on.campus.housing:mask.mandated.days + on.campus.housing:occupational.degree + 
    on.campus.housing:hs.equivalent.degree + gap20repub:private + 
    private:occupational.degree + percent.student.loan:mask.mandated.days + 
    percent.student.loan:hs.equivalent.degree + occupational.degree:hs.equivalent.degree, 
    data=df, family="poisson", weights=1/total.headcount)

back.step.summary = summary(back.step)

summary_pval <- back.step.summary$coefficients[,4]
significant_pval = c()
significant_coeff = c()
terms = c()

for (i in 2:length(summary_pval)){
  predictor_name = substr(names(summary_pval[i]), 1, 9)
  if (summary_pval[i] < 0.05 && predictor_name == "religious"){
    significant_pval = c(significant_pval, summary_pval[i])
    significant_coeff = c(significant_coeff, back.step.summary$coefficients[i, 1])
  }
}

p.compare.df <- data.frame("Gaussian Estimate"=significant_coeff, 
                           "Gaussian p.value"=significant_pval)
p.compare.df$coefficient <- rownames(p.compare.df)

cov <- vcovHC(back.step.p, type="HC0")
std.err <- sqrt(diag(cov))
r.est <- data.frame("Poisson Estimate"=unname(coef(back.step.p)), 
                    Poisson.p.value= 2 * pnorm(abs(coef(back.step.p)/std.err), lower.tail=FALSE))
r.est$coefficient <- rownames(r.est)

merge(p.compare.df, r.est, by="coefficient", all.x=TRUE)
```
\normalsize

Of the three `religious` interaction effects that were determined to be statistically significant in the Gaussian `back.step` model, only two remained statistically significant in the Poisson regression recreation of this model: `religiousYes:mask.mandated.days` and `religiousYes:on.campus.housingYes`. `religiousYes:log(percent.asian + epsilon)` was not statistically significant in the Poisson model. 

Overall, we have observed that most of the coefficients that were marked as statistically significant in the Gaussian models fit earlier in the paper remained statistically significant under the assumptions of Poisson regression. This corroborates our analyses and findings from earlier in the paper, and allows us to now draw the overarching conclusions of our analysis. 

# Conclusions




